# -*- coding: utf-8 -*-
"""TP1_agustina_7_2022_06_29.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12pLz0jL0gNwnLC4VjztkQ06t4KSRz2XP

<p align="center">
_____________________________________________________________________________________________________________________________

<br>
<br>
  <img src="https://www.estudiaradistancia.com.ar/logos/original/logo-universidad-de-buenos-aires.webp" height=180 />
  <img  src="https://confedi.org.ar/wp-content/uploads/2020/09/fiuba_logo.jpg" height="180">
<br>
_____________________________________________________________________________________________________________________________
<br>
<br>
<font size="+3">
[71.12] Análisis Numérico I
<br>
Trabajo Práctico I
<br>
Primer Cuatrimestre 2022
</font>
<br>
<br>
_____________________________________________________________________________________________________________________________
<br>
<br>
<font size="+2">
Desarrollo de métodos numéricos de busqueda de raíces e interpolacion
<br>
de Spline cúbica para su aplicación en problemas de optimización industrial. 
</font>
<br>
<br>
_____________________________________________________________________________________________________________________________
<br>
<br>
<font size="+2">
AUTORES
</font>
<br>
<font size="+1">
Gamberale Luciano Martin,
<br>  
Veiga Angel Martin,
<br>
Godoy Dupont Mateo,
<br>
Vasquez Jimenez Miguel Angel,
</font>
<br>
<br>
<font size="+2">
DOCENTES
</font>
<br>
<font size="+1">
Sassano Myriam Patricia
<br>
Garcia Ezequiel
<br>
Husain Santiago
<br>
Payva Matias
<br>
Turano Maria Agustina
<br>
Vera Ramiro
</font>
<br>
<br>
<br>

_____________________________________________________________________________________________________________________________
</p>

___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

# Índice

0. Introducción
    - 0.1 Objetivos
    - 0.2 Introducción
1. Plan de producción:
    - 1.1 Implementacion de metodos de busqueda de Raices:  
        - 1.1.1 Definiciones, funciones e imports auxiliares:
            - 1.1.1.1 Imports
            - 1.1.1.2 Calculo de orden de convergencia (α)
            - 1.1.1.3 Calculo de la constante asintótica (λ)
            - 1.1.1.4 Funciones de iteraciones funcionales
                - 1.1.1.4.1 Para métodos de semilla única
                - 1.1.1.4.2 Caso particular método Secante (semilla doble)
            - 1.1.1.5 Funciones para extraccion de datos una vez completada la aplicacion de los metodos para problemas especificos
                - 1.1.1.5.1 Extraccion de valores importantes tras biseccion
                - 1.1.1.5.2 Extraccion de valores importantes tras metodos de iteracion funcional
                - 1.1.1.5.3 Función auxiliar para representar los valores en tablas
                - 1.1.1.5.4 Extracción de valor obtenido en ultima iteración: pn
            - 1.1.1.6 Criterios de paro:
        - 1.1.2 Metodo de la biseccion
        - 1.1.3 Metodo Punto Fijo
        - 1.1.4 Metodo Newton-Raphson
        - 1.1.5 Metodo Newton-Raphson modificado para raices multiples
        - 1.1.6 Metodo Secante
    - 1.2 Planteo del problema
        - 1.2.1 (Pregunta a) Formule el problema y grafíquelo.
        - 1.2.2 (Pregunta b) Hallar las cantidades a fabricar del producto “E41” utilizando los métodos vistos en clase:
            - 1.2.2.1 Bisección/ Busqueda Binaria / Metodo de Arranque:
            - 1.2.2.2 Método del Punto Fijo:
            - 1.2.2.3 Método de Newton-Raphson:
            - 1.2.2.4 Método de Newton-Raphson modificado:
            - 1.2.2.5 Método de la Secante:
        - 1.2.3 (Pregunta c) Realizar una tabla con los resultados de las iteraciones, convergencia P y la constante asintótica λ.
            - 1.2.3.1 Método de Bisección
            - 1.2.3.2 Método de Punto Fijo
            - 1.2.3.3 Método de Newton-Raphson
            - 1.2.3.4 Método de Newton-Raphson Modificado
            - 1.2.3.5 Método de la Secante
        - 1.2.4 (Pregunta d) Compare los resultados obtenidos para los distintos métodos y cotas, grafique el orden de convergencia P y la constante asintótica λ para todos los casos. Realice un gráfico log10(Δx) vs iteraciones, para visualizar el comportamiento de la constante asintótica y el orden de convergencia. Discuta ventajas y desventajas. ¿Son las que esperaba en base a la teoría?
            - 1.2.4.1 Funciones auxiliares para plots:
            - 1.2.4.2 Graficos orden de convergencia α vs Iteraciones
            - 1.2.4.3 Graficos constante asintótica λ vs Iteraciones
            - 1.2.4.4 Graficos log10(Δx) vs Iteraciones
2. Splines para aproximacion de curvas
    - 2.1 Implementación del método de iterpolacion de Spline cúbica:
        - 2.1.1 Definiciones de funciones de cálculo de coeficientes:
        - 2.1.2 Definiciones de funciones auxiliares para ploteo:
        - 2.1.3 Metodo de interpolación de Spline cúbica
    - 2.2 Planteo del problema
        - 2.2.1 Uso de datos, interpolación y tabla de coeficientes resultantes:
        - 2.2.2 Gráfico
        - 2.2.3 Conclusiones

3. Conclusiones generales

4. Referencias

# ***0. Introducción***

### 0.1 Objetivos

El presente trabajo se divide en dos problemas a resolver:
- Plan de producción:
    - Se analiza la cantidad mínima requerida de un producto industrial a partir de dos restricciones principales relacionadas con la rentabilidad y la producción.
    - Se desarrolla el análisis por medio del uso de métodos numéricos para la obtención de raíces de funciones, obteniendo así aproximaciones de la cantidad mencionada anteriormente.
    - Se implementaron los algoritmos correspondientes a los siguientes métodos numéricos:
        - Bisección
        - Punto Fijo
        - Newton-Raphson
        - Newton-Raphson para raíces multiples
        - Secante
    - Se realizan análisis previos con el fin de garantizar el cumplimiento de las condiciones necesarias para aplicar cada uno de los métodos.
    - Se realizan análisis posteriores a la aplicación de los métodos y la correspondiente comparativa entre lo obtenido y lo esperado según las bases teóricas. 
    - Se analizan gráficamente los resultados los métodos, incluyendo propiedades de los mismos como son: constante asintótica y orden de convergencia. 
- Spline para aproximación de curvas:
    - Se busca realizar una aproximación de una función partida dado un conjunto de puntos de la misma y sus derivadas en los extremos. Para ello se utiliza la interpolación de Spline cúbica ligada.
    - Se implementan las funciones necesarias para realizar el cálculo de coeficientes independientes, lineales, cuadráticos y cúbicos mediante la forma matricial tridiagonal.
    - Se analizan gráficamente los resultados de las interpolaciones y se comparan con los puntos originales de la tabla.

### 0.2 Introducción

Inicialmente se investiga el funcionamiento y el desempeño de los distintos métodos numéricos aplicados a un problema particular.

Dicho problema consiste en la busqueda de la cantidad mínima requerida de producto dadas dos restricciones explicadas más adelante.

En conjunto con el valor mínimo esperado como respuesta al problema planteado, se espera corroborar que el desempeño de los métodos se corresponde a las propiedades mencionadas en el objetivo del trabajo para cada uno de ellos.

Posteriormente se investiga una aplicación práctica del método para interpolación de Spline cúbica ligada desarrollada por medio de su forma matricial cuya deducción se encuentra en su respectivo apartado en el cuerpo del trabajo.

Se espera que la aproximación resultante cumpla con todas las condiciones planteadas en la sección respectiva del cuerpo del trabajo y que se reflejen gráficamente.

# ***1. Plan de producción:***

Se le solicita el área "Supply Chain" que indique qué mix de productos es más conveniente
fabricar mensualmente dada una serie de restricciones que tiene la planta y la contribución
marginal que ofrece cada producto.

 Como nuevo pasante del área le asignan el cálculo para
el producto estrella de la empresa, acero de calidad “E41” utilizado para fabricar perfiles de
"Steel Frame".

Consulta con el área de Rentabilidad la utilidad unitaria del producto, y le informan que la
misma responde a la siguiente función: $$0,001·x·(x−1000kg
)^2$$, donde x es la cantidad de producto
a producir (medida en kilogramos), y que para producirse debe alcanzarse los $25000
de contribución mensual.

Luego, llama al área de Producción y le consulta por las restricciones que tiene la línea donde
se fabrica este producto.

Le informan que para que se justifique hacer el set up para prender
la máquina correspondiente, se deberán fabricar al menos $827 kg$ del mismo.

## ***1.1  Implementacion de metodos de busqueda de Raices:***

### **1.1.1 Definiciones, funciones e imports auxiliares:**

#### *1.1.1.1  Imports*
"""

import matplotlib.pyplot as plt
from matplotlib.ticker import (AutoMinorLocator, MultipleLocator) 
import numpy as np
from math import isclose
import pandas as pd

MAX_ITERACIONES = 30
MAX_VALOR_DE_ERROR = 1e+20
MIN_VALOR_DE_ERROR = 1e-12

ERROR_FUERA_DE_RANGO = "La cota del error es un valor muy cercano a cero no representable."
ERROR_MAX_ITERACIONES = "Raíz no encontrada: Se llegó al maximo de iteraciones sin encontrar la raiz."
ERROR_EJECUCION = "Error en los parametros al ejecutar el método."
ERROR_TAMAÑO_LISTA = "La listas recibidas deben poseer el mismo tamaño."
ERROR_DIVERGENCIA = "El método diverge para estos parámetros"

COTA_DE_ERROR_1 = 1e-05
COTA_DE_ERROR_2 = 1e-13

"""Tener en cuenta que para los calculos de orden de convergencia y constante asintótica se hace una aproximación en base a los límites que los definen. Entre mayor sea el número de iteraciones más precisa será esa aproximación.
Por definición se tenía:

$\displaystyle{\lim_{n \to \infty}}$ $ \frac{|p_{n+1}-p|}{|p_n-p|^{α}} = \lambda$

#### *1.1.1.2 Calculo de orden de convergencia ($\alpha$)*

$\alpha_n \cong \frac{log|({x_n-x_{n-1})/(x_{n-1} - x_{n-2})|}}{log|(x_{n-1}-x_{n-2})/(x_{n-2}-x_{n-3})|}$
"""

def calcular_orden_de_convergencia_alfa(lista_pn):
    lista_orden_de_convergencia = []

    for i in range(len(lista_pn)):
        alpha_i = None
        if ( (i > 2) and not(isclose((lista_pn[i]-lista_pn[i-1]) ,0, rel_tol = MIN_VALOR_DE_ERROR))):
            alpha_i = ((np.log(abs((lista_pn[i]-lista_pn[i-1])/(lista_pn[i-1]-lista_pn[i-2])))) / (np.log(abs((lista_pn[i-1]-lista_pn[i-2])/(lista_pn[i-2]-lista_pn[i-3])))))
        
        lista_orden_de_convergencia.append(alpha_i)
    return lista_orden_de_convergencia

"""#### *1.1.1.3 Calculo de la constante asintótica ($\lambda$)*

$\lambda_{n} \cong  \frac{\Delta x_n}{\Delta x_{n-1}^{\alpha_{n}}}$
"""

def calcular_cte_asintotica_lambda(lista_errores, lista_alfas):
    if(len(lista_errores) <= len(lista_alfas)-1):
        print(ERROR_TAMAÑO_LISTA)
        return None

    lista_cte_asintotica = []

    for i in range(len(lista_alfas)):
        lambda_i = None
        if(i > 2):
            if ((lista_errores[i-1] != None) and (lista_alfas[i] != None)) : # Condicion necesaria por metodo Secante
                lambda_i = lista_errores[i] / (lista_errores[i-1] ** lista_alfas[i])
            lista_cte_asintotica.append(lambda_i)
        else:
            lista_cte_asintotica.append(None)

    return lista_cte_asintotica

"""#### *1.1.1.4 Funciones de iteraciones funcionales*

###### 1.1.1.4.1 Para métodos de semilla única
$g(x) = x - 𝝋(x)f(x) $ ;

con $𝝋(x)$ definida dependiedo del método (Punto fijo, NR, NR modificado)
"""

def iteracion_funcional_recursiva(g, numero_iteracion, tolerancia, lista_resultados):
    if(numero_iteracion == MAX_ITERACIONES):  
        print(ERROR_MAX_ITERACIONES)
        return lista_resultados, False

    pn_1 = lista_resultados[numero_iteracion-1][0]
    pn = g(pn_1)

    error_actual = np.abs(pn_1 - pn)
    if(error_actual > MAX_VALOR_DE_ERROR):
        print(ERROR_DIVERGENCIA)
        return lista_resultados, False
    if (isclose(error_actual,0, rel_tol = MIN_VALOR_DE_ERROR) or error_actual == 0):             #Caso de que el error sea menor que la capacidad de la computadora
        print(ERROR_FUERA_DE_RANGO)
        error_actual =  MIN_VALOR_DE_ERROR
        lista_resultados_n = [pn, error_actual]  #[pn, error]  
        lista_resultados.append(lista_resultados_n)
        return lista_resultados, True

    lista_resultados_n = [pn, error_actual]  #[pn, error]  
    lista_resultados.append(lista_resultados_n)
                            
    if(error_actual < tolerancia):
        return lista_resultados, True

    return iteracion_funcional_recursiva(g, numero_iteracion+1, tolerancia, lista_resultados)

"""###### 1.1.1.4.2 Caso particular método Secante (semilla doble)
$g(x,y) = x - 𝝋(x,y)f(x)$ 
"""

def iteracion_funcional_recursiva_de_dos_semillas(g, numero_iteracion, tolerancia, lista_resultados):
    if(numero_iteracion == MAX_ITERACIONES): 
        return lista_resultados, False

    pn_1 = lista_resultados[numero_iteracion-1][0]
    pn_2 = lista_resultados[numero_iteracion-2][0]
    pn = g(pn_1,pn_2)
    

    error_actual = np.abs(pn - pn_1)
    if(error_actual > MAX_VALOR_DE_ERROR):
        print(ERROR_DIVERGENCIA)
        return lista_resultados, False
    if (isclose(error_actual,0, rel_tol = MIN_VALOR_DE_ERROR) or error_actual == 0):             #Caso de que el error sea menor que la capacidad de la computadora
        print(ERROR_FUERA_DE_RANGO)
        error_actual =  MIN_VALOR_DE_ERROR
        lista_resultados_n = [pn, error_actual]  #[pn, error]  
        lista_resultados.append(lista_resultados_n)
        return lista_resultados, True

    lista_resultados_n = [pn, error_actual]  # [pn, error]  
    lista_resultados.append(lista_resultados_n)
        
    if(error_actual < tolerancia):
        return lista_resultados, True

    return iteracion_funcional_recursiva_de_dos_semillas(g, numero_iteracion+1, tolerancia, lista_resultados)

"""#### *1.1.1.5 Funciones para extraccion de datos una vez completada la aplicacion de los metodos para problemas especificos*

###### 1.1.1.5.1 Extraccion de valores importantes tras biseccion 
Dichos valores son:

$p_n$ o de errores: $|p_n-p_{n-1}|$  de cada iteración a partir de la
lista con resultados totales del método de biseccion (exclusivamente)
"""

def extraer_pn_biseccion(lista_resultados):
    lista_xi = []

    for i in range(len(lista_resultados)):
        lista_xi.append(lista_resultados[i][2])

    return lista_xi



def extraer_errores_biseccion(lista_resultados):
    lista_xi = []

    for i in range(len(lista_resultados)):
        lista_xi.append(lista_resultados[i][3])

    return lista_xi

"""###### 1.1.1.5.2 Extraccion de valores importantes tras metodos de iteracion funcional
Dichos valores son:

$p_n$ o de errores: $|p_n-p_{n-1}|$  de cada iteración a partir de la
lista con resultados totales de cualquiera de los metodos con iteracion funcional
"""

def extraer_pn(lista_resultados):
    lista_xi = []

    for i in range(len(lista_resultados)):
        lista_xi.append(lista_resultados[i][0])

    return lista_xi



def extraer_errores(lista_resultados):
    lista_xi = []

    for i in range(len(lista_resultados)):
        lista_xi.append(lista_resultados[i][1])

    return lista_xi

"""###### *1.1.1.5.3 Función auxiliar para representar los valores en tablas*"""

def lista_resultados_a_dataframe(lista_resultados,lista_alfa,lista_cte_asintotica,columnas):
    lista_auxiliar = lista_resultados   
    for i in range(len(lista_auxiliar)):
        lista_auxiliar[i].append(lista_alfa[i])
        lista_auxiliar[i].append(lista_cte_asintotica[i])
    df = pd.DataFrame(lista_auxiliar, columns=columnas)
    if len(df) > 12:
        return pd.concat([df.head(), df.tail()])
    else:
        return df

"""###### 1.1.1.5.4 Extracción de valor obtenido en ultima iteración: $p_n$ """

def extraer_ultimo_pn_biseccion(lista_resultados):
    posicion_ultimo_elemento = len(lista_resultados)-1

    ultimo_elemento = lista_resultados[posicion_ultimo_elemento][2] 

    return ultimo_elemento


def extraer_ultimo_pn(lista_resultados):
    posicion_ultimo_elemento = len(lista_resultados)-1

    ultimo_elemento = lista_resultados[posicion_ultimo_elemento][0] 

    return ultimo_elemento

"""#### 1.1.1.6 Criterios de paro:
En general utilizamos el criterio de paro $ |p_n  - p_{n-1}| \leq ϵ$; con $ϵ = tolerancia$

A su vez tambien consideramos diversos casos de error o advertencias a lo largo de los algoritmos, como son:
- Divergencia de métodos (por lo tanto el error entre iteraciones es creciente).
- El valor aproximado de una raíz devuelto por el método en dos iteraciones consecutivas fue el mismo (por lo cual el error deja de ser representable)
- Se llega a una cantidad máxima de iteraciones antes de cumplir con la tolerancia pedida.

### **1.1.2 Metodo de la biseccion**

Su implementación se basa en que si $f \in C[a,b] $ y $f(a) ⋅ f(b) < 0$ ; el teoréma de Bolzano garantiza que existe $p \in (a,b)$ tal que $f(p) = 0$

Con esto se va dividiendo en subintervalos del intervalo $[a,b]$  y en cada paso se determina en qué subintervalo se encuentra la raíz usando el mismo razonamiento; repitiendo hasta que se llegue a los distintos criterios de paro.

(Tener en cuenta entonces que para el correcto funcionamiento del método se debe garantizar que $f(a) ⋅ f(b) < 0$)
"""

def biseccion_recursivo(funcion, numero_iteracion, an, bn, tolerancia, lista_resultados):
    if(MAX_ITERACIONES == numero_iteracion):
        print(ERROR_MAX_ITERACIONES) 
        return lista_resultados,False 

    pn = (an+bn)/2 
    error_actual = np.abs((bn-an)/2)
    lista_resultados_n = [an, bn, pn, error_actual]  #[an, bn, pn, error]  
    lista_resultados.append(lista_resultados_n)
                            
    if((error_actual < tolerancia) or (isclose(funcion(pn), 0))):
        return lista_resultados, True

    if(funcion(an) * funcion (pn) > 0):
        an = pn
    elif(funcion(bn) * funcion (pn) > 0):
        bn = pn

    return biseccion_recursivo(funcion, numero_iteracion+1, an, bn, tolerancia, lista_resultados) 


def biseccion(funcion, an, bn, tolerancia): 
    lista_resultados = []
    numero_de_iteracion = 0
    
    if((an >= bn) or (tolerancia <= 0) or (MAX_ITERACIONES <= 0)):
        print(ERROR_EJECUCION)
        return None, False

    return biseccion_recursivo(funcion, numero_de_iteracion, an, bn, tolerancia, lista_resultados)

"""#### Ejemplo de uso"""

f = lambda x:  x - np.e**-x
a0 = 0.1
b0 = 1

lista_resultados_biseccion = biseccion(f, a0, b0, tolerancia=1e-06)[0]
lista_pn_biseccion = extraer_pn_biseccion(lista_resultados_biseccion)
lista_errores_biseccion = extraer_errores_biseccion(lista_resultados_biseccion)
lista_alfa_biseccion = calcular_orden_de_convergencia_alfa(lista_pn_biseccion)
lista_cte_asintotica_biseccion = calcular_cte_asintotica_lambda(lista_errores_biseccion, lista_alfa_biseccion)



lista_resultados_a_dataframe(lista_resultados_biseccion,lista_alfa_biseccion,lista_cte_asintotica_biseccion,['an', 'bn', 'pn', 'error','alfa','lambda'])

"""### **1.1.3 Metodo Punto Fijo**

Su implementación se basa en el uso de una función $g(x)$ de iteración funcional, tal que 
$g(x) = x - f(x) $ ;

con $𝝋(x) = 1$ para éste método

Esto viene de que:
 
Queremos analizar el valor de la raíz de una $f \in C[a,b] $,
estamos buscando  $p \in (a,b)$ tal que $f(p) = 0$;

A su vez, sea una función $g \in C[a,b] $, se dice que la misma tiene punto fijo  $p \in [a,b]$ si $g(p) = p$;

Por lo tanto si se define $g(x) = x - f(x) $  y $p$ es punto fijo de $g$:

$g(p) = p = p - f(p)$

 $⇒$ $f(p) = p - p = 0$

Con lo cual hallar el punto fijo de $g$ es equivalente a hallar la raíz buscada de $f$

(Tener en cuenta entonces que para el correcto funcionamiento del método se debe garantizar que:
- Existe punto fijo para $g$ en el intervalo:
    
    Debe cumplirse que  $g \in C[a,b] $  y  $g(x) \in [a,b]$ $\forall$ $x \in [a,b]$
- El punto fijo es único para $g$ en el intervalo:
    
    Debe cumplirse que  $∃$ $g'(x)$ $\forall$ $x \in (a,b)$  y que  $∃$ $0 < k < 1$ tal que $\forall$ $x \in (a,b)$ vale que $|g'(x)| \leq k$

)
"""

def metodo_punto_fijo(funcion, p0, tolerancia):
    lista_resultados = []
    numero_iteracion = 0
    g = lambda x: x - funcion(x)

    if((tolerancia <= 0) or (MAX_ITERACIONES <= 0)):
        return None, False
        
    lista_resultados.append([p0, None])

    return iteracion_funcional_recursiva(g, numero_iteracion+1, tolerancia, lista_resultados)

"""#### Ejemplo de uso"""

f = lambda x:  x - np.e**-x
semilla_p0 = 0.55

lista_resultados_punto_fijo = metodo_punto_fijo(f, semilla_p0, tolerancia=1e-06)[0]
lista_pn_punto_fijo = extraer_pn(lista_resultados_punto_fijo)
lista_alfa_punto_fijo = calcular_orden_de_convergencia_alfa(lista_pn_punto_fijo)
lista_errores_punto_fijo = extraer_errores(lista_resultados_punto_fijo)
lista_cte_asintotica_punto_fijo = calcular_cte_asintotica_lambda(lista_errores_punto_fijo, lista_alfa_punto_fijo)

lista_resultados_a_dataframe(lista_resultados_punto_fijo,lista_alfa_punto_fijo,lista_cte_asintotica_punto_fijo,columnas=['pn', 'error','alfa','lambda'])

"""### **1.1.4 Metodo Newton-Raphson**

Su implementación se basa en el uso de una función $g(x)$ de iteración funcional, tal que 
$g(x) = x - \frac{f(x)}{f'(x)} $ ;

con $𝝋(x) = \frac{1}{f'(x)}$ para éste método

Esto viene de que:
 
Se realiza su deducción a partir del polinomio de taylor, tomando una aproximación de la raíz $p$ de tal forma que la distancia de dicha aproximación a la raíz real sea cercana a cero.

En base a esta "cercanía" se puede deducir que el cumplimiento de las hipótesis de punto fijo se satisfacen bajo dicha condición.

(Tener en cuenta entonces que para el correcto funcionamiento del método se debe garantizar que:
- La semilla o aproximación inicial debe ser elegida en un intervalo tal que se le considere suficientemente a la raíz esperada.
- Debe cumplirse que  $f \in C^2[a,b] $  y  $f'(x)$ no debe anularse en el entorno inicial que se elija para contener a la semilla y a la raíz

)
"""

def metodo_newton_raphson(funcion, derivada_funcion, p0, tolerancia):
    lista_resultados = []
    numero_iteracion = 0
    g = lambda x: x - funcion(x)/derivada_funcion(x)

    if((tolerancia <= 0) or (MAX_ITERACIONES <= 0)):
        return ERROR_EJECUCION

    lista_resultados.append([p0, None])

    return iteracion_funcional_recursiva(g, numero_iteracion+1, tolerancia, lista_resultados)

"""#### Ejemplo de uso"""

f = lambda x:  x - np.e**-x
derivada_f = lambda x:  1 + np.e**-x
semilla_p0 = 0.55

lista_resultados_NR = metodo_newton_raphson(f, derivada_f, semilla_p0, tolerancia=1e-06)[0]
lista_pn_NR = extraer_pn(lista_resultados_NR)
lista_alfa_NR = calcular_orden_de_convergencia_alfa(lista_pn_NR)
lista_errores_NR = extraer_errores(lista_resultados_NR)
lista_cte_asintotica_NR = calcular_cte_asintotica_lambda(lista_errores_NR, lista_alfa_NR)

lista_resultados_a_dataframe(lista_resultados_NR,lista_alfa_NR,lista_cte_asintotica_NR,['pn', 'error','alfa','lambda'])

"""### **1.1.5 Metodo Newton-Raphson modificado para raices multiples**

Su implementación se basa en el uso de una función $g(x)$ de iteración funcional, tal que 
$g(x) = x - \frac{f(x)⋅f'(x)}{f'(x)^2 - f(x)⋅f''(x)} $ ;

con $𝝋(x) = \frac{f'(x)}{f'(x)^2 - f(x)⋅f''(x)}$ para éste método

Esto viene de que:
 
Se realiza la modificación a partir de ver que al aplicar NR sin modificar para raices de multiplicidad algebráica $m > 1,$ hace que se pierda la convergencia cuadrática del método, con lo cual se modifica teniendo en cuenta que:

$f \in C^m[a,b] $  tiene un cero de multiplicidad algebráica $m$ en $p \in (a,b)$ $⇔$ $0 = f(p) = f'(p) = f''(p) = ⋯ = f^{(m-1)}(p)$ y $f^{(m)}(p) \neq 0$

(Tener en cuenta entonces que para el correcto funcionamiento del método se deben garantizar las mismas condiciones mencionadas para Newton-Raphson)
"""

def metodo_newton_raphson_para_raices_multiples(funcion, derivadaFuncion, derivadaSegundaFuncion, p0, tolerancia):
    lista_resultados = []
    numero_iteracion = 0
    g = lambda x: x - (funcion(x)*derivadaFuncion(x))/(derivadaFuncion(x)**2 - funcion(x)*derivadaSegundaFuncion(x))

    if((tolerancia <= 0) or (MAX_ITERACIONES <= 0)):
        return None, False

    lista_resultados.append([p0, None])

    return iteracion_funcional_recursiva(g, numero_iteracion+1, tolerancia, lista_resultados)

"""#### Ejemplo de uso"""

f = lambda x:  x - np.e**-x
derivada_f = lambda x:  1 + np.e**-x
derivada_segunda_f = lambda x:  -np.e**-x
semilla_p0 = 0.55

lista_resultados_NR_multiple = metodo_newton_raphson_para_raices_multiples(f, derivada_f, derivada_segunda_f, semilla_p0, tolerancia=1e-06)[0]
lista_pn_NR_multiple = extraer_pn(lista_resultados_NR_multiple)
lista_alfa_NR_multiple = calcular_orden_de_convergencia_alfa(lista_pn_NR_multiple)
lista_errores_NR_multiple = extraer_errores(lista_resultados_NR_multiple)
lista_cte_asintotica_NR_multiple = calcular_cte_asintotica_lambda(lista_errores_NR_multiple, lista_alfa_NR_multiple)

lista_resultados_a_dataframe(lista_resultados_NR_multiple,lista_alfa_NR_multiple,lista_cte_asintotica_NR_multiple,['pn', 'error','alfa','lambda'])

"""### **1.1.6 Metodo Secante**

Su implementación se basa en el uso de una función $g(x)$ de iteración funcional, tal que 
$g(x,y) = x - \frac{f(x)⋅(x-y)}{f(x) - f(y)} $ ;

con $𝝋(x,y) = \frac{(x-y)}{f(x) - f(y)}$ para éste método

(Para mayor claridad, se muestra la sucesión como:
    $p_{n} = p_{n-1} - \frac{f(p_{n-1})⋅(p_{n-1}-p_{n-2})}{f(p_{n-1}) - f(p_{n-2})}$
)

Esto viene de que:

Se usa la definición de derivada con el cociente incremental para llegar a una aproximación que permite evitar la evaluación de $f'$ proveniente del método de NR.

(Tener en cuenta que, a pesar de que se usa una aproximación de los valores de la derivada, para el correcto funcionamiento del método se deben garantizar las mismas condiciones mencionadas para Newton-Raphson. Además de esto ahora se debe contar con **dos semillas** o aproximaciones iniciales cercanas a la raíz buscada)
"""

def metodo_secante(funcion, p0, p1, tolerancia):
    lista_resultados = []
    numero_iteracion = 0
    g = lambda x,y: x - (funcion(x)*(x-y))/(funcion(x)-funcion(y))

    if((tolerancia <= 0) or (MAX_ITERACIONES <= 0)):
        return ERROR_EJECUCION

    lista_resultados.append([p0, None])
    lista_resultados.append([p1, None])
    
    return iteracion_funcional_recursiva_de_dos_semillas(g, numero_iteracion+2, tolerancia, lista_resultados)

"""#### Ejemplo de uso"""

f = lambda x:  x - np.e**-x
semilla_p0 = 0
semilla_p1 = 5

lista_resultados_secante = metodo_secante(f, semilla_p0, semilla_p1, tolerancia=1e-06)[0]
lista_pn_secante = extraer_pn(lista_resultados_secante)
lista_alfa_secante = calcular_orden_de_convergencia_alfa(lista_pn_secante)
lista_errores_secante = extraer_errores(lista_resultados_secante)
lista_cte_asintotica_secante = calcular_cte_asintotica_lambda(lista_errores_secante, lista_alfa_secante)

lista_resultados_a_dataframe(lista_resultados_secante,lista_alfa_secante,lista_cte_asintotica_secante,['pn', 'error','alfa','lambda'])

"""## ***1.2 Planteo del problema***

###1.2.1 (Pregunta $a$) Formule el problema y grafíquelo.

____________________________________________________________________________
 **Datos del problema:**
 - $827$ $kg$ es el minimo de mercaderia para justificar el setup de las maquinas.
 - $\$25000$ es el ingreso minimo que se desea obtener.
____________________________________________________________________________

Se tiene que la utilidad unitaria corresponde a la expresión:

  $u(x) = 0,001\frac{$}{kg^3}·x·(x−1000kg)^2$

Por lo tanto para cumplir con la contribucion minima que se solicita $(\$25000)$ se busca analizar las cantidades de producto correspondiente al proponer:

 $\$25000 \leq u(x) = 0,001\frac{$}{kg^3}·x·(x−1000kg)^2$

$⇒$ $0 \leq 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$

Por lo tanto con el fin de buscar la cantidad de producto mínima para cumplir con la restricción de contribución mínima mensual y justificación del setup de las máquinas, plantearemos la siguiente función a analizar:

 $f(x) = 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$

Dado que para justificar el setup de las máquinas se necesitan al menos $827$ $kg$ $⇒$ $x \geq 827$ $kg$ 

En resumen, buscamos el $x_{min}$ tal que $f(x_{min}) \geq 0 $  ‎ ‎ $,$ ‎‎ ‎ ‎ $x_{min} \geq 827$ $kg$

A continuación realizaremos un análisis gráfico de $f(x)$


La función anteriormente planteada es una función cubica, por lo cual tendrá un máximo de 3 raices.
"""

MIN_CONTRIBUCION_MENSUAL = 25000

def utilidad_unitaria (x):
  return 0.001*x*((x-1000)**2)

def f(x):
  return utilidad_unitaria(x)-MIN_CONTRIBUCION_MENSUAL

intervalo = np.linspace(-50,1400,1000)
func = f(intervalo)

fig, ax = plt.subplots(figsize=(10, 10))

plt.title("Grafica  f(x) vs x ")
plt.xlabel("Cantidad de producto (x) [kg]")
plt.ylabel("f(x) [$]")

plt.plot(intervalo, func, label = 'f(x)', color = 'blue', linewidth= 2)

plt.plot([-200,1400],[0,0], label = 'ejes', color = 'black')
plt.plot([0,0],[-50000, 130000], color = 'black')
plt.plot([827,827],[-50000, 130000], label = 'x = 827 [kg]', color = 'red', linestyle='--')


ax.set_xlim(-200, 1400)
ax.set_ylim(-50000, 130000)

ax.xaxis.set_major_locator(MultipleLocator(100))

ax.grid()

plt.legend()
plt.show()

"""____________________________________________________________________________
  **Información actual**:
  - $f(x) = 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$
  - Buscamos el $x_{min}$ tal que $f(x_{min}) \geq 0 $  ‎ ‎ $,$ ‎‎ ‎ ‎ $x_{min} \geq 827$ $kg$
____________________________________________________________________________

Se observa que la función posee tres raices:
- $(1)$ Acotada en intervalo $x$ $\in$ $[0,100]$
- $(2)$ Acotada en intervalo $x$ $\in$ $[800,900]$
- $(3)$ Acotada en intervalo $x$ $\in$ $[1100,1200]$


 **Análisis de raíces**:

- *Análisis raíz $(1)$*: 

    A partir de las tres raíces podemos descartar el análisis de la raíz $(1)$ más cercana a $x=0$ dado que la misma se puede acotar en el intervalo $[0,100]$, lo cuál implica que ésta cantidad de producto no justifica el setup de las máquinas (siempre se cumple que esa cantidad es menor a $827$ $kg$). 

- *Análisis raíz $(2)$*: 

    Dado que debe cumplirse $x \geq 827$ $kg$, tomaremos el intervalo $[827,900]$ y verificaremos el comportamiento de la función en dicho intervalo para saber si allí existe una raíz ya que con el análisis gráfico no tenemos suficiente certeza:
    - Tomamos un $x$ $\in$ $[827,900]$
    - $⇒ 827 \leq x \leq 900 $
    - $⇒ -173 \leq x - 1000 \leq -100 $
    - $⇒ 29929 \leq (x - 1000)^2 \leq 10000 $
    - $⇒ 0.001⋅29929 \leq 0.001⋅(x - 1000)^2 \leq 0.001⋅10000 $
    - $⇒ 827⋅(0.001⋅29929) \leq x⋅(0.001⋅29929) \leq 0.001⋅x⋅(x - 1000)^2$ $;$
      $ 80.001⋅x⋅(x - 1000)^2  \leq x⋅(0.001⋅10000) \leq 900⋅(0.001⋅10000) $
    - $⇒ 827⋅(0.001⋅29929) \leq 0.001⋅x⋅(x - 1000)^2 \leq 900⋅(0.001⋅10000) $
    - $⇒ 827⋅(0.001⋅29929)-25000 \leq 0.001⋅x⋅(x - 1000)^2 - 25000 \leq 900⋅(0.001⋅10000) - 250000 $
    - $⇒ 827⋅0.001⋅29929-25000 \leq 0.001⋅x⋅(x - 1000)^2 - 25000 \leq 900⋅0.001⋅10000 - 250000 $
    - $⇒ -248.717 \leq 0.001⋅x⋅(x - 1000)^2 - 25000 \leq -16000 $
    - $⇒ -248.717 \leq f(x) \leq -16000 $
    - $⇒ f(x) \leq 0$ $∀$ $x$ $\in$ $[827,900]$

    Teniendo esto en cuenta, determinamos que no se cumple una de las restricciones pedidas ($f(x) \geq 0 $) y además no existe ninguna raiz dentro de ese intervalo. 

    Dado que la raiz $x \in [800,900], x \notin [800,900] ⇒ x \in [800,827)$, lo cual conlleva que que la raiz $(2)$ tambien sea descartada siendo que la cantidad de producto no justifica el setup de las máquinas (siempre se cumple que esa cantidad es menor a $827$ $kg$). 

- *Análisis raíz $(3)$*: 

    En cuanto a la raíz restante procederemos a aplicar los métodos de búsqueda de raíces siendo que no se puede acotar visualmente para conocer su valor y asi obtener la cantidad mínima de producto a generar.

### 1.2.2 (Pregunta $b$) Hallar las cantidades a fabricar del producto “E41” utilizando los métodos vistos en clase:
### Bisección, Punto Fijo, Newton-Raphson, Newton-Raphson modificado y Secante.
### Use para todos los métodos como criterio de parada las siguientes cotas:
### $1 · 10^{−5}$ y $1 · 10^{−13}$, use como semilla un valor tomado con el criterio que considere correcto, justificar.

#### **1.2.2.1 Bisección/ Busqueda Binaria / Metodo de Arranque:**

##### *Definiciones previas*

$f(x) = 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$

##### *Verificación de la convergencia del método*

A partir del Teorema de Bolzano o Teorema del valor medio es que demostramos la existencia de al menos una raiz dentro del intervalo $[1100,1200]$:

Dado que $f(x) = 0,001\frac{$}{kg^3}·x·(x−1100kg)^2 - \$25000$ es un polinomio de grado 3, sabemos que $f \in  C[1100;1200]$. 

A su vez, verificamos que $f(1100) ⋅ f(1200) < 0$ que junto al analisis grafico anterior nos permite confirmar la existencia de una única raiz en dicho intervalo.

##### *Aplicación del método* 
Tras realizar la comprobación de las hipotesis mencionadas aplicamos el metodo de bisección.
"""

f(1200)*f(1100) < 0

lista_resultados_biseccion_cota_1, encontrado_biseccion_cota_1 = biseccion(f, 1100,1200, tolerancia=COTA_DE_ERROR_1)

extraer_ultimo_pn_biseccion(lista_resultados_biseccion_cota_1)

"""Dado que el método no imprimió ningun error y además el valor está dentro del intervalo $[1100; 1200]$, junto con lo anteriormente expuesto concluimos que la cantidad de producto mínima para cumplir con la restricción de contribución mínima mensual y la justificación del setup de las máquinas con la tolerancia de $1\cdot10^{-5}$ es: 

$x = [1147.59629 \pm 0.00001]kg$

"""

lista_resultados_biseccion_cota_2, encontrado_biseccion_cota_2 = biseccion(f, 1100,1200, tolerancia=COTA_DE_ERROR_2)

extraer_ultimo_pn_biseccion(lista_resultados_biseccion_cota_2)

"""Si bien no se encontró un valor con tolerancia $1\cdot10^{-13}$ se puede proponer el valor de la última iteración del método: 

$x = [1147.5962886 \pm 0.0000001] kg$

(Valor del error obtenido de tabla de resultados)

#### **1.2.2.2 Método del Punto Fijo:**

##### *Definiciones previas*

Siendo que buscamos las raices de la funcion $f(x)$, es decir, $f(x) = 0$, propongo una función $g(x) = x - f(x)$ a la cual le aplicaré el metodo del punto fijo. Esto es porque suponiendo que la raiz de $f(x)$ sea el valor $p ⇒ g(p) = p - f(p) ⇒ g(p) = p - 0$ ⇒ $g(p) = p$

Definimos $g(x) = x - f(x) = x - 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$

##### *Verificación de la convergencia del método*

Verificamos existencia y unicidad de la raiz en en intervalo cerrado $[1100,1200]$. 
Para ello se deben cumplir las siguientes condiciones:
1.   Si $g(x) ∈ C[1100,1200]$ y $g(x) ∈ [1100,1200]$ $∀$ $x ∈ [1100,1200]$ ⇒ existe al menos un punto fijo dentro del intervalo $[1100,1200]$
2.   Si $∃$ $g'(x)$ $∀$  $x ∈ (1100,1200)$ y $|g'(x)| ≤ k$ $∀$  $x ∈ (1100,1200)$ donde $0 < k < 1$ ⇒ el punto fijo es único dentro del intervalo $[1100,1200]$

En caso de cumplir con ambas condiciones, la funcion $g(x)$ es admisible y por lo tanto (por teorema visto en clase), $∀$ $p_0 \in [1100,1200]$ la sucesión definida como $p_n = g(p_{n-1})$ converge al único punto fijo de $g(x)$.

###### *Verifico condición 1:*

Dado que $g(x) = x - f(x)$ y $f(x) ∈ C[1100,1200]$ por ser un polinomio de grado 3 ⇒ $g(x) ∈ C[1100,1200]$ por ser suma de funciones $∈ C[1100,1200]$.

A continuacion verificaremos gráficamente si se cumple que $g(x) ∈ [1100,1200]$ $∀$ $x ∈ [1100,1200]$ (Condición 1):
"""

def g(x,funcion):
  return x-funcion(x)

intervalo = np.linspace(-100,1400,1000)
func = g(intervalo,f)

fig, ax = plt.subplots(figsize=(10, 10))

plt.title("Grafica función g(x)")
plt.xlabel("x")
plt.ylabel("y")

plt.plot(intervalo, func, label = 'g(x)', color = 'blue', linewidth= 2)

plt.plot([-200,1400],[0,0], label = 'ejes', color = 'black')
plt.plot([0,0],[-50000, 130000], color = 'black')

plt.plot([1100,1200],[1100,1100], label = 'Condición 1', color = 'red', linestyle='--')
plt.plot([1100,1200],[1200,1200], color = 'red', linestyle='--')
plt.plot([1100,1100],[1100,1200], color = 'red', linestyle='--')
plt.plot([1200,1200],[1100,1200], color = 'red', linestyle='--')

ax.set_xlim(-100, 1400)
ax.set_ylim(-100, 1400)

ax.xaxis.set_major_locator(MultipleLocator(100))
ax.yaxis.set_major_locator(MultipleLocator(100))

ax.grid()
plt.legend(loc = 'best')
plt.show()

"""Podemos concluir al mirar el gráfico que la condición $g(x) ∈ [1100,1200]$ para cualquier $x ∈ [1100,1200]$ no se cumple, ya que la función cruza las lineas rojas tanto superiormente como inferiormente. En caso de cumplirse la condición, la función deberia cruzar por las lineas laterales. 

Esto implica que no se puede asegurar la existencia de un punto fijo dentro de dicho intevalo.

###### *Verifico condición 2:*

Siendo que no podemos asegurar la existencia de un punto fijo dentro del intervalo $[1100,1200]$ mediante la condicion 1, ya no se cumple que la función $g(x)$ sea admisible.

Sin embargo, verificaremos de todas formas si se cumple la segunda condición:

Dado que $g(x) = x - 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$, entonces 
$g'(x) = 1 - 0,001\frac{$}{kg^3} ·(x−1000kg)^2 - 2 \cdot 0,001\frac{$}{kg^3} ⋅ x ⋅ (x−1000kg)$
"""

def g_derivada(x):
  return 1-0.001*((x-1000)**2)-2*0.001*x*(x-1000)

intervalo = np.linspace(950,1250,100)
func = g_derivada(intervalo)

fig, ax = plt.subplots(figsize=(10, 10))

plt.title("Grafica función g'(x)")
plt.xlabel("x")
plt.ylabel("y")

plt.plot(intervalo, func, label = "g'(x)", color = 'blue', linewidth= 2)
plt.plot([950,1250],[0,0],"black")
plt.plot([950,1250],[1,1], label = 'y =  1', color = 'red', linestyle='--')
plt.plot([950,1250],[-1,-1], label = 'y = -1', color = 'red', linestyle='--')

plt.plot([1100,1100],[-10,10], label = 'x = 1100 [kg]', color = 'grey', linestyle='--', linewidth= 2)
plt.plot([1200,1200],[-10,10], label = 'x = 1200 [kg]', color = 'grey', linestyle='--', linewidth= 2)

ax.set_xlim(950, 1250)
ax.set_ylim(-10, 10)

ax.yaxis.set_major_locator(MultipleLocator(1))

plt.grid()
plt.legend()
plt.show()

"""$g(x)$ tampoco cumple con la condición 2: $|g'(x)| ≤ k$ con $0 < k < 1$ en el intervalo $[1100;1200]$.

##### *Aplicación del método* 

Pese a saber que el método no converge bajo estas condiciones, se muestra que al intentar obtener la raiz se llega al maximo de iteraciones permitidas:

La semilla $p_0$ se obtiene a partir de la primer iteracion del método de la biseccion, en donde anteriormente se demostró que dicho método convergerá a la raiz.

Tomando el intervalo $[1100, 1200]$ ⇒ $p_0 = \frac{(a + b)}{2} = \frac{(1100 \cdot kg + 1200 \cdot kg)}{2} = 1150 kg$
"""

semilla_p0 = 1150

lista_resultados_punto_fijo_cota_1, encontrado_punto_fijo_cota_1 = metodo_punto_fijo(f, semilla_p0, tolerancia=COTA_DE_ERROR_1)

lista_resultados_punto_fijo_cota_2, encontrado_punto_fijo_cota_2 = metodo_punto_fijo(f, semilla_p0, tolerancia=COTA_DE_ERROR_2)

"""Dado que no se cumplieron las hipótesis para aplicar el método, se corrobora que diverge para ambas cotas de error y por lo tanto no se puede obtener el valor de cantidad de producto mínimo correspondiente.

#### **1.2.2.3 Método de Newton-Raphson:**

##### *Definiciones previas*

$f(x) = 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$

##### *Verificación de la convergencia del método*

A partir del teorema visto en clase que se citará a continuacion, verificarmemos si se puede asegurar la convergencia de este método:

Sea $f \in C^2[a,b]$. Si $p \in [a,b]$ tal que $f(p) = 0$ y $f'(p) \neq 0$ entonces $∃$ $𝛅 > 0$ tal que el método de Newton-Raphson genera una sucesión : { ${p_n}$ }$_{n \geq 1}$ que converge a $p$ para cualquier aproximación inicial $p_0 \in [p-𝛅, p+𝛅]$ 

Dado que $f$ es un polinomio de grado 3 $⇒$ $f \in C^2[1100,1200]$

Derivo $f$ para verificar que no se anule la derivada en dicho intervalo:

$f'(x) = 0,001\frac{$}{kg^3} ·(x−1000kg)^2 + 2 \cdot 0,001\frac{$}{kg^3} ⋅ x ⋅ (x−1000kg)$
"""

def f_derivada_primera(x):
  return 0.001*((x-1000)**2)+2*0.001*x*(x-1000)

intervalo = np.linspace(1050,1250,100)
derivada = f_derivada_primera(intervalo)

fig, ax = plt.subplots(figsize=(10, 10))

plt.title("Grafica función f'(x)")
plt.xlabel("x")
plt.ylabel("y")

plt.plot(intervalo, derivada, label = "f'(x)", color = 'blue', linewidth= 2)

plt.plot([1050,1250],[0,0], label = 'eje x', color = 'black')

plt.plot([1100,1100],[-300,600], label = 'x = 1100 [kg]', color = 'grey', linestyle='--', linewidth= 2)
plt.plot([1200,1200],[-300,600], label = 'x = 1200 [kg]', color = 'grey', linestyle='--', linewidth= 2)

ax.set_xlim(1050, 1250)
ax.set_ylim(-300, 600)

plt.grid()
plt.legend(loc = 'best')
plt.show()

"""Como podemos observar en el gráfico, $f'(x) \neq 0$ $∀$ $x \in [1100, 1200]$, lo cual implica que el método converge a la raiz dentro de dicho intervalo.

##### *Aplicación del método* 

La semilla $p_0$ se obtiene a partir de la primer iteracion del método de la biseccion, en donde anteriormente se demostró que dicho método convergerá a la raiz.

Tomando el intervalo $[1100, 1200]$ ⇒ $p_0 = \frac{(a + b)}{2} = \frac{(1100 \cdot kg + 1200 \cdot kg)}{2} = 1150 kg$
"""

semilla_p0 = 1150
lista_resultados_NR_cota_1, encontrado_NR_cota_1 = metodo_newton_raphson(f, f_derivada_primera, semilla_p0, tolerancia=COTA_DE_ERROR_1)

extraer_ultimo_pn(lista_resultados_NR_cota_1)

"""Dado que el método no imprimió ningun error y además el valor está dentro del intervalo $[1100; 1200]$, junto con lo anteriormente expuesto concluimos que la cantidad de producto mínima para cumplir con la restricción de contribución mínima mensual y la justificación del setup de las máquinas con la tolerancia de $1\cdot10^{-5}$ es: 

$x = [1147.59629 \pm 0.00001]kg$

"""

lista_resultados_NR_cota_2, encontrado_NR_cota_2 = metodo_newton_raphson(f, f_derivada_primera, semilla_p0, tolerancia=COTA_DE_ERROR_2)

extraer_ultimo_pn(lista_resultados_NR_cota_2)

"""Dado que el error entre dos iteraciones resultó $|p_n - p_{n-1}| = 0$, no hubo diferencia entre el valor de las aproximaciones consecutivas de la raíz.
Esto se puede explicar debido a la cantidad de digitos significativos con la que trabaja la computadora, por lo cual concluimos que:

Dado que el método no imprimió ningun error y además el valor está dentro del intervalo $[1100; 1200]$, junto con lo anteriormente expuesto concluimos que la cantidad de producto mínima para cumplir con la restricción de contribución mínima mensual y la justificación del setup de las máquinas con la tolerancia de $1\cdot10^{-12}$ es: 

$x = [1147.596288534586 \pm 10^{-12}]kg$

#### **1.2.2.4 Método de Newton-Raphson modificado:**

##### *Definiciones previas*

$f(x) = 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$ 

$f'(x) = 0,001\frac{$}{kg^3} ·(x−1000kg)^2 + 2 \cdot 0,001\frac{$}{kg^3} ⋅ x ⋅ (x−1000kg)$

##### *Verificación de la convergencia del método*

A partir del analsis realizado para el método de Newton-Raphson:

Dado que $f$ es un polinomio de grado 3 $⇒$ $f \in C^2[1100,1200]$

$ f(x) = 0$ $∀$ $x \in [1100, 1200]$, 

$f'(x) \neq 0$ $∀$ $x \in [1100, 1200]$

Podemos concluir que $f$ tiene un cero simple en el intervalo $[1100, 1200]$

Dado que la multiplicidad de la raíz de $f$ no modifica la convergencia cuadrática que posee el método, solo resta indicar que la funcion $g$ definida de la siguiente manera, es continua:

$g(x) = x - \frac{f(x) \cdot f'(x)}{[f'(x)]^2 - f(x) ⋅ f''(x)}$

Defino: $ϕ(x) = \frac{f(x) \cdot f'(x)}{[f'(x)]^2 - f(x) ⋅ f''(x)}$

$⇒ g(x) = x - ϕ(x)$

Por lo tanto debemos verificar que la funcion $ϕ$ sea continua dentro del intervalo $[1100, 1200]$.

Para verificar la continuidad de $ϕ$ debo ver en que punto se anula la expresion: $[f'(x)]^2 - f(x) ⋅ f''(x)$ ya que el numerador de $ϕ(x) = \frac{f(x) \cdot f'(x)}{[f'(x)]^2 - f(x) ⋅ f''(x)}$ es una funcion continua, dado que $f$ es un polinomio de grado (multiplicacion de funciones continuas da como resultado una funcion continua).

Busco $f''(x)$:

$f'(x) = 0,001\frac{$}{kg^3} ·(x−1000kg)^2 + 2 \cdot 0,001\frac{$}{kg^3} ⋅ x ⋅ (x−1000kg)$

$f''(x) = 2 ⋅ 0,001\frac{$}{kg^3} ·(x−1000kg) + 2 \cdot 0,001\frac{$}{kg^3} ⋅ (x−1000kg) + 2 \cdot 0,001\frac{$}{kg^3} ⋅ x $

$f''(x) = 2 ⋅ 0,001\frac{$}{kg^3} · [2 \cdot (x−1000kg) +  x ]$

$f''(x) = 2 ⋅ 0,001\frac{$}{kg^3} · (3x−2000kg)$

$f''(x) = 0.006\frac{$}{kg^3} · x − 4 \frac{$}{kg^2} $

Grafico la funcion $α(x) = [f'(x)]^2 - f(x) ⋅ f''(x)$  en el intervalo $[1100, 1200]$ para ver si posee raices:

$α(x) = [0,001\frac{$}{kg^3} ·(x−1000kg)^2 + 2 \cdot 0,001\frac{$}{kg^3} ⋅ x ⋅ (x−1000kg)] ^ 2 - (0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000) ⋅ (0,001\frac{$}{kg^3} ·(x−1000kg)^2 + 2 \cdot 0,001\frac{$}{kg^3} ⋅ x ⋅ (x−1000kg))$
"""

def f_derivada_segunda(x):
  return 0.006*x - 4

def alpha(f, f_derivada_primera, f_derivada_segunda, x):
    return ((f_derivada_primera(x)) ** 2) - (f(x) * f_derivada_segunda(x))

intervalo = np.linspace(1050,1250,100)
func = alpha(f, f_derivada_primera, f_derivada_segunda, intervalo)

fig, ax = plt.subplots(figsize=(10, 10))

plt.title("Grafica función f'(x)")
plt.xlabel("x")
plt.ylabel("y")

plt.plot(intervalo, func, label = "$α(x)$", color = 'blue', linewidth= 2)

plt.plot([1050,1250],[0,0], label = 'eje x', color = 'black')

plt.plot([1100,1100],[-25000,200000], label = 'x = 1100 [kg]', color = 'grey', linestyle='--', linewidth= 2)
plt.plot([1200,1200],[-25000,200000], label = 'x = 1200 [kg]', color = 'grey', linestyle='--', linewidth= 2)

ax.set_xlim(1050, 1250)
ax.set_ylim(-25000, 200000)

plt.grid()
plt.legend(loc = 'best')
plt.show()

"""Dado que la funcion $α$ no contiene raices en el intervalo $[1100, 1200]$ $⇒ ϕ$ es continua dentro de dicho intervalo $⇒ g$ es continua dentro de dicho intervalo.

Esto implica que la convergencia de este método esta asegurada y ademas es cuadrática.

##### *Aplicación del método* 

La semilla $p_0$ se obtiene a partir de la primer iteracion del método de la biseccion, en donde anteriormente se demostró que dicho método convergerá a la raiz.

Tomando el intervalo $[1100, 1200]$ ⇒ $p_0 = \frac{(a + b)}{2} = \frac{(1100 \cdot kg + 1200 \cdot kg)}{2} = 1150 kg$
"""

semilla_p0 = 1150

lista_resultados_NR_multiple_cota_1, encontrado_NR_multiple_cota_1 = metodo_newton_raphson_para_raices_multiples(f, f_derivada_primera, f_derivada_segunda, semilla_p0, tolerancia=COTA_DE_ERROR_1)

extraer_ultimo_pn(lista_resultados_NR_multiple_cota_1)

"""Dado que el método no imprimió ningun error y además el valor está dentro del intervalo $[1100; 1200]$, junto con lo anteriormente expuesto concluimos que la cantidad de producto mínima para cumplir con la restricción de contribución mínima mensual y la justificación del setup de las máquinas con la tolerancia de $1\cdot10^{-5}$ es: 

$x = [1147.59629 \pm 0.00001]kg$

"""

lista_resultados_NR_multiple_cota_2, encontrado_NR_multiple_cota_2 = metodo_newton_raphson_para_raices_multiples(f, f_derivada_primera, f_derivada_segunda, semilla_p0, tolerancia=COTA_DE_ERROR_2)

extraer_ultimo_pn(lista_resultados_NR_multiple_cota_2)

"""Dado que el error entre dos iteraciones resultó $|p_n - p_{n-1}| = 0$, no hubo diferencia entre el valor de las aproximaciones consecutivas de la raíz.
Esto se puede explicar debido a la cantidad de digitos significativos con la que trabaja la computadora, por lo cual concluimos que:

Dado que el método no imprimió ningun error y además el valor está dentro del intervalo $[1100; 1200]$, junto con lo anteriormente expuesto concluimos que la cantidad de producto mínima para cumplir con la restricción de contribución mínima mensual y la justificación del setup de las máquinas con la tolerancia de $1\cdot10^{-12}$ es: 

$x = [1147.596288534586 \pm 10^{-12}]kg$

#### **1.2.2.5 Método de la Secante:**

##### *Definiciones previas*

$f(x) = 0,001\frac{$}{kg^3}·x·(x−1000kg)^2 - \$25000$

Dado que el método de la Secante proviene del método de Newton-Raphson, con el objetivo de no utilizar $f'(x)$, realizando la siguiente aproximación:

$f'(p_{n-1}) ≅ \frac{f(p_{n-2}) - f(p_{n-1})}{p_{n-2} - p_{n-1}}$ 

Es por esto que el método de la Secante posee las mismas condiciones de convergencia que el método de Newton-Raphson.

Gracias al análisis realizado en el método de Newton-Raphson en donde:

Dado que $f$ es un polinomio de grado 3 $⇒$ $f \in C^2[1100,1200]$

$ f(x) = 0$ $∀$ $x \in [1100, 1200]$, 

$f'(x) \neq 0$ $∀$ $x \in [1100, 1200]$

Entonces podemos concluir que el método de la Secante convergerá a la unica raiz contenida en el intervalo $[1100, 1200]$.

##### *Aplicación del método* 

La semilla $p_0$ se obtiene a partir de la primer iteracion del método de la biseccion, en donde anteriormente se demostró que dicho método convergerá a la raiz.

Tomando el intervalo $[1100, 1200]$ ⇒ $p_0 = \frac{(a + b)}{2} = \frac{(1100 \cdot kg + 1200 \cdot kg)}{2} = 1150 kg$

La semilla $p_1$ también la voy a obtener a partir del método de la bisección, pero con la segunda iteración.

Para esto verifico cual es mi nuevo intervalo:

$f(1150) = 0 ⇒ x = 1150kg$ es la raíz de $f$.  

$f(1100) \cdot f(1150) < 0 ⇒$ el nuevo intervalo es $[1100, 1150]$

$f(1150) \cdot f(1200) < 0 ⇒$ el nuevo intervalo es $[1150, 1200]$
"""

f(1100)*f(1150) < 0

f(1150)*f(1200) < 0

"""Observando los resultados obtenidos, el nuevo intervalo resultante será $[1100, 1150]$.

Por lo tanto la segunda semilla del método de la Secante será la siguiente:

$p_1 = \frac{(a + b)}{2} = \frac{(1100 \cdot kg + 1150 \cdot kg)}{2} = 1125 kg$
"""

semilla_p0 = 1150
semilla_p1 = 1125

lista_resultados_secante_cota_1, encontrado_secante_cota_1 = metodo_secante(f, semilla_p0, semilla_p1, tolerancia=COTA_DE_ERROR_1)

extraer_ultimo_pn(lista_resultados_secante_cota_1)

"""Dado que el método no imprimió ningun error y además el valor está dentro del intervalo $[1100; 1200]$, junto con lo anteriormente expuesto concluimos que la cantidad de producto mínima para cumplir con la restricción de contribución mínima mensual y la justificación del setup de las máquinas con la tolerancia de $1\cdot10^{-5}$ es: 

$x = [1147.59629 \pm 0.00001]kg$

"""

lista_resultados_secante_cota_2, encontrado_secante_cota_2 = metodo_secante(f, semilla_p0, semilla_p1, tolerancia=COTA_DE_ERROR_2)

extraer_ultimo_pn(lista_resultados_secante_cota_2)

"""Dado que el error entre dos iteraciones resultó $|p_n - p_{n-1}| = 0$, no hubo diferencia entre el valor de las aproximaciones consecutivas de la raíz.
Esto se puede explicar debido a la cantidad de digitos significativos con la que trabaja la computadora, por lo cual concluimos que:

Dado que el método no imprimió ningun error y además el valor está dentro del intervalo $[1100; 1200]$, junto con lo anteriormente expuesto concluimos que la cantidad de producto mínima para cumplir con la restricción de contribución mínima mensual y la justificación del setup de las máquinas con la tolerancia de $1\cdot10^{-12}$ es: 

$x = [1147.596288534586 \pm 10^{-12}]kg$

### 1.2.3 (Pregunta $c$) Realizar una tabla con los resultados de las iteraciones, convergencia $P$ y la constante asintótica $\lambda$. 
### En caso de que se encuentren más de 12 iteraciones, solamente incluir en la tabla las primeras 5 iteraciones y luego las últimas 5.

En las siguientes tablas se muestran los valores correspondientes a todos los resultados obtenidos por cada método hasta encontrar la raíz con la precisión pedida, o en su defecto hasta que el método se detenga por alguna de las siguientes causas:
- Máxima cantidad de iteraciones (Decidida en 30 en base a los resultados generales).
- Valor máximo de error entre iteraciones (en el caso en el que un método diverge).
- Valor mínimo de error entre iteraciones (en el caso en el que el error actual no sea aritmeticamente operable por ser muy pequeño).

Siendo que la computadora trabaja con una cantidad finita de dígitos y la cota del error dada ($1\cdot10^{-13}$) la supera, ocurre que dos aproximaciones consecutivas de la raíz poseen el mismo valor, lo cual no implica que se haya encontrado la raíz exacta sino que es una limitación de representación.

Esto conlleva a que la constante asintótica y el orden de convergencia no puedan ser calculados para aproximaciones de raíz con mismo valor como sucede en la última iteración.

Esto sucede en la aplicación de los métodos: 
- NR.
- NR modificado.
- Secante.

#### 1.2.3.1 Método de Bisección
"""

lista_pn_biseccion_cota_1 = extraer_pn_biseccion(lista_resultados_biseccion_cota_1)
lista_errores_biseccion_cota_1 = extraer_errores_biseccion(lista_resultados_biseccion_cota_1)
lista_alfa_biseccion_cota_1 = calcular_orden_de_convergencia_alfa(lista_pn_biseccion_cota_1)
lista_cte_asintotica_biseccion_cota_1 = calcular_cte_asintotica_lambda(lista_errores_biseccion_cota_1, lista_alfa_biseccion_cota_1)

tabla_de_resultados = lista_resultados_a_dataframe(lista_resultados_biseccion_cota_1,lista_alfa_biseccion_cota_1,lista_cte_asintotica_biseccion_cota_1,['an', 'bn', 'pn', 'error','alfa','lambda'])
tabla_de_resultados

lista_pn_biseccion_cota_2 = extraer_pn_biseccion(lista_resultados_biseccion_cota_2)
lista_errores_biseccion_cota_2 = extraer_errores_biseccion(lista_resultados_biseccion_cota_2)
lista_alfa_biseccion_cota_2 = calcular_orden_de_convergencia_alfa(lista_pn_biseccion_cota_2)
lista_cte_asintotica_biseccion_cota_2 = calcular_cte_asintotica_lambda(lista_errores_biseccion_cota_2, lista_alfa_biseccion_cota_2)

tabla_de_resultados = lista_resultados_a_dataframe(lista_resultados_biseccion_cota_2,lista_alfa_biseccion_cota_2,lista_cte_asintotica_biseccion_cota_2,['an', 'bn', 'pn', 'error','alfa','lambda'])
tabla_de_resultados

"""####  1.2.3.2 Método de Punto Fijo"""

lista_pn_punto_fijo_cota_1 = extraer_pn(lista_resultados_punto_fijo_cota_1)
lista_alfa_punto_fijo_cota_1  = calcular_orden_de_convergencia_alfa(lista_pn_punto_fijo_cota_1)
lista_errores_punto_fijo_cota_1  = extraer_errores(lista_resultados_punto_fijo_cota_1)
lista_cte_asintotica_punto_fijo_cota_1  = calcular_cte_asintotica_lambda(lista_errores_punto_fijo_cota_1, lista_alfa_punto_fijo_cota_1 )

tabla_de_resultados = lista_resultados_a_dataframe(lista_resultados_punto_fijo_cota_1,lista_alfa_punto_fijo_cota_1,lista_cte_asintotica_punto_fijo_cota_1,columnas=['pn', 'error','alfa','lambda'])
tabla_de_resultados

lista_pn_punto_fijo_cota_2 = extraer_pn(lista_resultados_punto_fijo_cota_2)
lista_alfa_punto_fijo_cota_2  = calcular_orden_de_convergencia_alfa(lista_pn_punto_fijo_cota_2)
lista_errores_punto_fijo_cota_2  = extraer_errores(lista_resultados_punto_fijo_cota_2)
lista_cte_asintotica_punto_fijo_cota_2  = calcular_cte_asintotica_lambda(lista_errores_punto_fijo_cota_2, lista_alfa_punto_fijo_cota_2 )

tabla_de_resultados = lista_resultados_a_dataframe(lista_resultados_punto_fijo_cota_2,lista_alfa_punto_fijo_cota_2,lista_cte_asintotica_punto_fijo_cota_2,columnas=['pn', 'error','alfa','lambda'])
tabla_de_resultados

"""####  1.2.3.3 Método de Newton-Raphson"""

lista_pn_NR_cota_1 = extraer_pn(lista_resultados_NR_cota_1)
lista_alfa_NR_cota_1 = calcular_orden_de_convergencia_alfa(lista_pn_NR_cota_1)
lista_errores_NR_cota_1 = extraer_errores(lista_resultados_NR_cota_1)
lista_cte_asintotica_NR_cota_1 = calcular_cte_asintotica_lambda(lista_errores_NR_cota_1, lista_alfa_NR_cota_1)

tabla_de_resultados = lista_resultados_a_dataframe(lista_resultados_NR_cota_1,lista_alfa_NR_cota_1,lista_cte_asintotica_NR_cota_1,['pn', 'error','alfa','lambda'])
tabla_de_resultados

lista_pn_NR_cota_2 = extraer_pn(lista_resultados_NR_cota_2)
lista_alfa_NR_cota_2 = calcular_orden_de_convergencia_alfa(lista_pn_NR_cota_2)
lista_errores_NR_cota_2 = extraer_errores(lista_resultados_NR_cota_2)
lista_cte_asintotica_NR_cota_2 = calcular_cte_asintotica_lambda(lista_errores_NR_cota_2, lista_alfa_NR_cota_2)

tabla_de_resultados = lista_resultados_a_dataframe(lista_resultados_NR_cota_2,lista_alfa_NR_cota_2,lista_cte_asintotica_NR_cota_2,['pn', 'error','alfa','lambda'])
tabla_de_resultados

"""####  1.2.3.4 Método de Newton-Raphson Modificado"""

lista_pn_NR_multiple_cota_1 = extraer_pn(lista_resultados_NR_multiple_cota_1)
lista_alfa_NR_multiple_cota_1 = calcular_orden_de_convergencia_alfa(lista_pn_NR_multiple_cota_1)
lista_errores_NR_multiple_cota_1 = extraer_errores(lista_resultados_NR_multiple_cota_1)
lista_cte_asintotica_NR_multiple_cota_1 = calcular_cte_asintotica_lambda(lista_errores_NR_multiple_cota_1, lista_alfa_NR_multiple_cota_1)

lista_resultados_a_dataframe(lista_resultados_NR_multiple_cota_1,lista_alfa_NR_multiple_cota_1,lista_cte_asintotica_NR_multiple_cota_1,['pn', 'error','alfa','lambda'])

lista_pn_NR_multiple_cota_2 = extraer_pn(lista_resultados_NR_multiple_cota_2)
lista_alfa_NR_multiple_cota_2 = calcular_orden_de_convergencia_alfa(lista_pn_NR_multiple_cota_2)
lista_errores_NR_multiple_cota_2 = extraer_errores(lista_resultados_NR_multiple_cota_2)
lista_cte_asintotica_NR_multiple_cota_2 = calcular_cte_asintotica_lambda(lista_errores_NR_multiple_cota_2, lista_alfa_NR_multiple_cota_2)

lista_resultados_a_dataframe(lista_resultados_NR_multiple_cota_2,lista_alfa_NR_multiple_cota_2,lista_cte_asintotica_NR_multiple_cota_2,['pn', 'error','alfa','lambda'])

"""#### 1.2.3.5 Método de la Secante"""

lista_pn_secante_cota_1 = extraer_pn(lista_resultados_secante_cota_1)
lista_alfa_secante_cota_1 = calcular_orden_de_convergencia_alfa(lista_pn_secante_cota_1)
lista_errores_secante_cota_1 = extraer_errores(lista_resultados_secante_cota_1)
lista_cte_asintotica_secante_cota_1 = calcular_cte_asintotica_lambda(lista_errores_secante_cota_1, lista_alfa_secante_cota_1)

lista_resultados_a_dataframe(lista_resultados_secante_cota_1,lista_alfa_secante_cota_1,lista_cte_asintotica_secante_cota_1,['pn', 'error','alfa','lambda'])

lista_pn_secante_cota_2 = extraer_pn(lista_resultados_secante_cota_2)
lista_alfa_secante_cota_2 = calcular_orden_de_convergencia_alfa(lista_pn_secante_cota_2)
lista_errores_secante_cota_2 = extraer_errores(lista_resultados_secante_cota_2)
lista_cte_asintotica_secante_cota_2 = calcular_cte_asintotica_lambda(lista_errores_secante_cota_2, lista_alfa_secante_cota_2)

lista_resultados_a_dataframe(lista_resultados_secante_cota_2,lista_alfa_secante_cota_2,lista_cte_asintotica_secante_cota_2,['pn', 'error','alfa','lambda'])

"""### 1.2.4 (Pregunta $d$) Compare los resultados obtenidos para los distintos métodos y cotas, grafique el orden de convergencia $P$ y la constante asintótica $\lambda$ para todos los casos. Realice un gráfico $log_{10}(\Delta x)$ vs iteraciones, para visualizar el comportamiento de la constante asintótica y el orden de convergencia. Discuta ventajas y desventajas. ¿Son las que esperaba en base a la teoría?

#### 1.2.4.1 Funciones auxiliares para plots:
"""

def plotear_puntos_vs_iter_log(lista_valores_en_y,color, label):
    label_ploteado = False
    for x in range(len(lista_valores_en_y)):
        if lista_valores_en_y[x] != None:
            if label_ploteado:
                plt.plot(x,np.log10(lista_valores_en_y[x]), marker="o", markersize=5, color=color)
            else:
                plt.plot(x,np.log10(lista_valores_en_y[x]), marker="o", markersize=5, color=color, label = label)
                label_ploteado = True


    

def plotear_puntos_vs_iter(lista_valores_en_y,color, label):
    label_ploteado = False
    for x in range(len(lista_valores_en_y)):
        if lista_valores_en_y[x] != None:
            if label_ploteado:
                plt.plot(x,lista_valores_en_y[x], marker="o", markersize=5, color=color)
            else:
                plt.plot(x,lista_valores_en_y[x], marker="o", markersize=5, color=color, label = label)
                label_ploteado = True

"""####  1.2.4.2 Graficos orden de convergencia $α$ vs Iteraciones

##### Con cota de error 1:
"""

fig, ax = plt.subplots(figsize=(20, 10))

plt.title("Grafica orden de convergencia alpha vs Iteraciones")
plt.xlabel("Iteraciones")
plt.ylabel("α")

plotear_puntos_vs_iter(lista_alfa_biseccion_cota_1,'red', "biseccion")
plotear_puntos_vs_iter(lista_alfa_punto_fijo_cota_1,'green', "punto fijo")
plotear_puntos_vs_iter(lista_alfa_NR_cota_1,'blue', "NR")
plotear_puntos_vs_iter(lista_alfa_NR_multiple_cota_1,'orange', "NR modif")
plotear_puntos_vs_iter(lista_alfa_secante_cota_1,'purple',"secante" )

ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(1))
ax.set_xlim(0, 30)
ax.set_ylim(0, 41)

ax.grid()
plt.legend()
plt.show()

"""Para ver con mayor precisión los valores exceptuando el primer valor del método de la secante (debido a su lejanía respecto de los demás valores en el gráfico anterior):"""

fig, ax = plt.subplots(figsize=(20, 10))

plt.title("Grafica orden de convergencia alpha vs Iteraciones")
plt.xlabel("Iteraciones")
plt.ylabel("α")

plotear_puntos_vs_iter(lista_alfa_biseccion_cota_1,'red', "biseccion")
plotear_puntos_vs_iter(lista_alfa_punto_fijo_cota_1,'green', "punto fijo")
plotear_puntos_vs_iter(lista_alfa_NR_cota_1,'blue', "NR")
plotear_puntos_vs_iter(lista_alfa_NR_multiple_cota_1,'orange', "NR modif")
plotear_puntos_vs_iter(lista_alfa_secante_cota_1,'purple',"secante" )


ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(0.2))
ax.set_xlim(0, 30)
ax.set_ylim(0, 5)

ax.grid()
plt.legend()
plt.show()

"""##### Con cota de error 2:"""

fig, ax = plt.subplots(figsize=(20, 10))

plt.title("Grafica orden de convergencia alpha vs Iteraciones")
plt.xlabel("Iteraciones")
plt.ylabel("α")

plotear_puntos_vs_iter(lista_alfa_biseccion_cota_2,'red', "biseccion")
plotear_puntos_vs_iter(lista_alfa_punto_fijo_cota_2,'green', "punto fijo")
plotear_puntos_vs_iter(lista_alfa_NR_cota_2,'blue', "NR")
plotear_puntos_vs_iter(lista_alfa_NR_multiple_cota_2,'orange', "NR modif")
plotear_puntos_vs_iter(lista_alfa_secante_cota_2,'purple', "secante")

ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(1))
ax.set_xlim(0, 30)
ax.set_ylim(0, 41)


ax.grid()
plt.legend()
plt.show()

"""Para ver con mayor precisión los valores exceptuando el primer valor del método de la secante (debido a su lejanía respecto de los demás valores en el gráfico anterior):"""

fig, ax = plt.subplots(figsize=(20, 10))

plt.title("Grafica orden de convergencia alpha vs Iteraciones")
plt.xlabel("Iteraciones")
plt.ylabel("α")

plotear_puntos_vs_iter(lista_alfa_biseccion_cota_1,'red', "biseccion")
plotear_puntos_vs_iter(lista_alfa_punto_fijo_cota_1,'green', "punto fijo")
plotear_puntos_vs_iter(lista_alfa_NR_cota_1,'blue', "NR")
plotear_puntos_vs_iter(lista_alfa_NR_multiple_cota_1,'orange', "NR modif")
plotear_puntos_vs_iter(lista_alfa_secante_cota_1,'purple',"secante" )


ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(0.2))
ax.set_xlim(0, 30)
ax.set_ylim(0, 5)

ax.grid()
plt.legend()
plt.show()

"""##### Conclusiones:
- En cuanto a orden de convergencia podemos ver que la bisección siempre permanece con valor constante por como se van realizando las divisiones de subintervalos en cada iteración, y queda claro que efectivamente tiene convergencia lineal.
- En cuanto al orden de convergencia de la secante, vemos que la estimación del valor del orden de convergencia va oscilando y tiende a converger aproximadamente en 1.5, lo cual confirma que para este caso se tiene una convergencia supralineal.
- En cuanto al orden de convergencia de Newton-Raphson y NR modificado vemos que, como habíamos visto en el análisis gráfico, la raíz era simple y por lo tanto ambos métodos coinciden en su orden de convergencia cuadrático, lo cual se ve rapidamente en el cálculo de $\alpha$ para los dos.
- En cuanto al orden de convergencia del punto fijo, los valores resultantes directamente no tienen sentido ya que el método en sí diverge para cualquier semilla que se elija en el intervalo mencionado antes.

####  1.2.4.3 Graficos constante asintótica $\lambda$ vs Iteraciones

##### Con cota de error 1:
"""

fig, ax = plt.subplots(figsize=(20, 10))

plt.title("Grafica constante asintótica $\lambda$ vs Iteraciones")
plt.xlabel("Iteraciones")
plt.ylabel("$\lambda$")

plotear_puntos_vs_iter(lista_cte_asintotica_biseccion_cota_1,'red', "biseccion")
plotear_puntos_vs_iter(lista_cte_asintotica_punto_fijo_cota_1,'green', "punto fijo")
plotear_puntos_vs_iter(lista_cte_asintotica_NR_cota_1,'blue', "NR")
plotear_puntos_vs_iter(lista_cte_asintotica_NR_multiple_cota_1,'orange', "NR modif")
plotear_puntos_vs_iter(lista_cte_asintotica_secante_cota_1,'purple', "secante")


ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(0.1))
ax.set_xlim(0, 30)
ax.set_ylim(-0.2, 1.3)


ax.grid()
plt.legend()

plt.show()

"""##### Con cota de error 2:"""

fig, ax = plt.subplots(figsize=(20, 10))

plt.title("Grafica constante asintótica $\lambda$ vs Iteraciones")
plt.xlabel("Iteraciones")
plt.ylabel("$\lambda$")

plotear_puntos_vs_iter(lista_cte_asintotica_biseccion_cota_2,'red', "biseccion")
plotear_puntos_vs_iter(lista_cte_asintotica_punto_fijo_cota_2,'green', "punto fijo")
plotear_puntos_vs_iter(lista_cte_asintotica_NR_cota_2,'blue', "NR")
plotear_puntos_vs_iter(lista_cte_asintotica_NR_multiple_cota_2,'orange', "NR modif")
plotear_puntos_vs_iter(lista_cte_asintotica_secante_cota_2,'purple', "secante")

ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(0.1))
ax.set_xlim(0, 30)
ax.set_ylim(-0.2, 1.3)

ax.grid()
plt.legend()

plt.show()

"""##### Conclusiones:
- Se observa que para el punto fijo los valores de $λ$ son practicamente cero, pero hay que tener en claro que esos valores no tienen sentido ya que si se hubiera podido calcular una iteración más (si se hubiese podido representar), su valor sería mucho mayor a 1 de donde se nota la divergencia.
- El método de la bisección tiene $λ$ constante $0.5$ y tiene sentido ya que siempre el error va disminuyendo en la misma razón ya que siempre se subdividen los intervalos a la mitad.
- En los métodos de NR el $λ$ es muy cercano a cero, pero en estos casos si tiene sentido ya que se llega a la precisión deseada muy rapidamente.
- Para el método de la secante $λ$ no toma valores tan cercanos a cero al principio (como los de NR, por lo tanto tiene que reaizar más iteraciones), pero aún así los mismos terminan siendo cercanos a cero en pocas.

####  1.2.4.4 Graficos  $log_{10}(\Delta x)$ vs Iteraciones

##### Con cota de error 1:
"""

fig, ax = plt.subplots(figsize=(20, 10))

plt.title("Grafica  $Log(\Delta x)$ vs Iteraciones")
plt.xlabel("Iteraciones")
plt.ylabel("$Log(\Delta x)$")

plotear_puntos_vs_iter_log(lista_errores_biseccion_cota_1,"red" ,"biseccion")
plotear_puntos_vs_iter_log(lista_errores_punto_fijo_cota_1,"green" ,"punto fijo")
plotear_puntos_vs_iter_log(lista_errores_NR_cota_1,"blue" ,"NR")
plotear_puntos_vs_iter_log(lista_errores_NR_multiple_cota_1,"orange" ,"NR modif")
plotear_puntos_vs_iter_log(lista_errores_secante_cota_1,"purple" ,"secante")

ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(2))
ax.set_xlim(-1, 30)
ax.set_ylim(-10, 35)

ax.grid()
plt.legend()

plt.show()

"""##### Con cota de error 2:"""

fig, ax = plt.subplots(figsize=(20, 10))

plt.title("Grafica  $Log(\Delta x)$ vs Iteraciones")
plt.xlabel("Iteraciones")
plt.ylabel("$Log(\Delta x)$")

plotear_puntos_vs_iter_log(lista_errores_biseccion_cota_2,"red","biseccion")
plotear_puntos_vs_iter_log(lista_errores_punto_fijo_cota_2,"green","punto fijo")
plotear_puntos_vs_iter_log(lista_errores_NR_cota_2,"blue","NR")
plotear_puntos_vs_iter_log(lista_errores_NR_multiple_cota_2,"orange","NR modif")
plotear_puntos_vs_iter_log(lista_errores_secante_cota_2,"purple" ,"secante")



ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(2))
ax.set_xlim(-1, 30)
ax.set_ylim(-10, 35)

ax.grid()
plt.legend()

plt.show()

"""##### Conclusiones:
- Se puede observar a partir de los gráficos que el método de la bisección es el más lento en llegar a la cota de error dada por la tolerancia.
- Luego, se observa que los métodos de NR y NR modificado fueron los más eficientes, ya que con muy pocas iteraciones llegaron a ambas cotas de error dadas por las tolerancias.  
- Acá tambien se puede ver cómo diverge el método de punto fijo en pocas iteraciones.
- A su vez el método de la secante presenta resultados esperados ya que llega a las cotas de error dadas por las tolerancias en una cantidad de iteraciones menor a la de la bisección, pero mayor a las de los métodos de NR, dado su comportamiento supralineal.

___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

# ***2. Splines para aproximacion de curvas***

## ***2.1 Implementación del método de iterpolacion de Spline cúbica:***

Dada una función $f$ definida en [a,b] y un conjunto de nodos: $x_0 < x_1 < ... < x_n$.

Se define trazadror cúbico o Spline Cúbica a la funcion $S(x)$ que satisface las condiciones siguientes:

1. $S(x)$ es un polinomio cúbico, denotado $S_i(x)$, en el subintervalo $[x_i,x_{i+1}]$ para cada $i=0,1,...,n-1$.
2. $S(x_i) = f(x_i)$  $\forall$  $i=0,1,...,n$.
3. $S_{i+1}(x_{i+1})=S_i(x_{i+1})$  $\forall$  $i=0,1,...,n-2$.
4. $S'_{i+1}(x_{i+1})=S'_i(x_{i+1})$  $\forall$  $i=0,1,...,n-2$
5. $S''_{i+1}(x_{i+1})=S''_i(x_{i+1})$  $\forall$  $i=0,1,...,n-2$
6. Una de las siguientes condiciones de frontera se satisface:
   1. $S''(x_0)=S''(x_n)$ frontera libre o natural.
   2. $S'(x_0)=f'(x_0)$ y $S'(x_n)=f'(x_n)$  frontera ligada o condicionada.

En la consigna se pide trabajar con la frontera ligada por lo tanto utilizaremos la última condición para el ejercicio.

### **2.1.1 Definiciones de funciones de cálculo de coeficientes:**

*(Notación: Denominamos **subsplines** a las particiones de las Splines según su definición en los subintervalos)*

Para construir una spline definida en un intervalo que se dividió en $n$ subintervalos requiere determinar $4n$ constantes. Para construir el spline cúbico para una función dada f, las condiciones en la definición se aplican a los polinomios cúbicos.

$S_i(x) = a_i + b_i (x − x_i) + c_i (x − x_i )^2 + d_i (x − x_i )^3$,

para cada $i = 0,1,...,n-1$. Puesto que $S_i(x_i) = a_i = f(x_i)$, la condición **3.** se puede aplicar para obtener

$a_{i+1} = S_{i+1}(x _{i+1}) = S_i (x _{i+1}) = a_i + b_i(x_{i+1} − x_i ) + c_i(x_{i+1} − x _i )^2 + d_i (x _{i+1} − x _i )^3$,

para cada $i= 0,1,...,n-2$.

Como los términos $x_{i+1} - x_i$ son usados muchas veces, por lo tanto es conveniente una notación mas simple

$h_i = x_{i+1} - x_i$

para cada  $i = 0,1,...,n-1$

Para esta notación, creamos la siguiente función que devuelve una lista de todos los elementos $h_i$

h_lista $= [h_1,h_2,...,h_{n-1}]$
"""

def crear_lista_de_incrementos_hi(tabla_de_datos_sobre_nodos: list):
    h_lista = []
    # voy de i = 0 a n-1
    for i in range(len(tabla_de_datos_sobre_nodos)-1):
        h_i = tabla_de_datos_sobre_nodos[i+1][0] - tabla_de_datos_sobre_nodos[i][0]
        h_lista.append(h_i)
    return h_lista

"""También definimos $a_n = f(x_n)$, entonces tenemos:

$a_{i+1} = a_i = b_i h_i +c_i h_i² + d_i h_i³$

para cada $i = 0,1,...,n-1$

Para organizar los datos, utilizamos la siguiente funcion donde a partir de los datos que tenemos, simplemente guardamos los puntos donde sabemos los valores de $f(x_n)$ en una lista *a_lista*
"""

def crear_lista_de_coeficientes_independientes_ai(tabla_de_datos_sobre_nodos: list):
    a_lista = []
    for i in range(len(tabla_de_datos_sobre_nodos)):
        a_i = tabla_de_datos_sobre_nodos[i][1] 
        a_lista.append(a_i)
    return a_lista

"""
Apartir de:

$S_i'(x) = b_i + 2c_i (x − x_i ) + 3d_i (x − x_i )^2$;

$S_i'(x_i) = b_i$  $\forall$  $ i = 0, 1,..., n-1 $

y las condiciones 4. y 5. Se puede desarrollar para llegar a expresar los coeficientes lineales de la forma:

$b_i = \frac{1}{h_i} (a_{i+1} - a_i) - \frac{h_i}{3} (2c_i +  c_{i+1})$"""

def crear_lista_de_coeficientes_lineales_bi(h_lista: list, a_lista: list, c_lista: list):
    nro_sub_splines = len(h_lista)
    lista_bi = []

    for i in range(nro_sub_splines):
        b_i = ((1/h_lista[i]) * (a_lista[i+1] - a_lista[i])) - ((h_lista[i]/3) * (2 * c_lista[i] + c_lista[i+1]))
        lista_bi.append(b_i)

    return lista_bi

"""De aquí, reduciendo en uno el indice y reemplazando los $b_i$ se llega a:

$h_{i-1}c_{i-1} + 2(h_{i-1} + h_i)c_i + h_i c_{i+1} =  \frac{3}{h_i} (a_{i+1} - a_i) - \frac{3}{h_{i-1}} (a_i - a_{i-1})$

Este sistema contiene solo los $\{c_i\}_{i=0}^n$ como incógnitas, ya que los valores de $\{h_i\}_{i=0}^n$ y de $\{a_i\}_{i=0}^n$ son conocidos, los $h$ son el espaciado entre los nodos y los $a$ son los valores de la función en los nodos.

**Teorema**: Si $f$ se define en $a= x_0 < x_1 < ... < x_b = b$ y es diferenciable en $a$ y $b$, entonces $f$ tiene un único spline interpolante $S$ condicionado en los nodos $x_0, x_1,..., x_n$; es decir, un spline interpolante que satisface las condiciones de frontera condicionada $S'(a) = f'(a) y $S'(b) = f'(b)$

**Demostración**: Para verificar que la ecuación anterior se cumple, utilizamos que como: $f'(a) = S'(a) = S'(x_0) = b_0$, y desarrollando la ecuacion vista antes $b_i = \frac{1}{h_i} (a_{i+1} - a_i) - \frac{h_i}{3} (2c_i +  c_{i+1})$ con $i = 0$ llegamos a:

$2h_0c_0 + h_0c_1 = \frac{3}{h_0}(a_1-a_0)-3f'(a)$

y con $i=n-1$:

$h_{n-1}c_{n-1} + 2h_{n-1}c_n = 3f'(b) - \frac{3}{h_{n-1}}(a_n-a_{n-1})$

con todos estos datos se puede determinar el sistema lineal $Ax = b$

$ 
A = 
\begin{pmatrix}
2h_0 & h_0 & 0 & ... & ... & 0 \\
h_0 & 2(h_0 + h_1) & h_1 & ... & ... & ...\\
0 & h_1 & 2(h_1 + h_2) & h_2 & ... & ... \\
... & ... & ... & ... & ... & 0 \\
... & ... & ... & h_{n-2} & 2(h_{n-2} + h_{n-1}) & h_{n-1} \\
0 & ... & ... & 0 & h_{n-1} & 2h_{n-1} \\
\end{pmatrix} 
;
$



$
b = 
\begin{pmatrix}
\frac{3}{h_0} (a_1 - a_0) - 3f'(x_0) \\
\frac{3}{h_1} (a_2 - a_1) - \frac{3}{h_0} (a_1 - a_0) \\
... \\ 
\frac{3}{h_{n-1}} (a_n - a_{n-1}) - \frac{3}{h_{n-2}} (a_{n-1} - a_{n-2}) \\
3f'(x_n) - \frac{3}{h_{n-1}} (a_n - a_{n-1})
\end{pmatrix}
;
$
"""

def matriz_A(h_lista: list):
    nro_sub_splines = len(h_lista)

    matriz_A = [([0]*(nro_sub_splines+1)) for _ in range(nro_sub_splines+1)]

    for i in range(nro_sub_splines+1):
        for j in range(nro_sub_splines+1):
            if i == j:
                if i == 0:
                    matriz_A[i][j] = 2 * h_lista[i]
                    matriz_A[i][j+1] = h_lista[i]
                elif i == nro_sub_splines:
                    matriz_A[i][j] = 2 * h_lista[i-1]
                    matriz_A[i][j-1] = h_lista[i-1]
                else:
                    matriz_A[i][j-1] = h_lista[i-1]
                    matriz_A[i][j] = 2 * (h_lista[i-1] + h_lista[i])
                    matriz_A[i][j+1] = h_lista[i]

    return matriz_A

def matriz_b(h_lista: list, a_lista: list, derivada_f_x0, derivada_f_xn):
    nro_sub_splines = len(h_lista)
    b = []

    for i in range(nro_sub_splines+1):
        if i == 0:
            b.append((3/h_lista[i] * (a_lista[i+1] - a_lista[i])) - (3 * derivada_f_x0))
        elif i == nro_sub_splines:
            b.append((3 * derivada_f_xn) - (3/h_lista[i-1] * (a_lista[i] - a_lista[i-1])))
        else:
            b.append(((3/h_lista[i]) * (a_lista[i+1] - a_lista[i])) - ((3/h_lista[i-1]) * (a_lista[i] - a_lista[i-1])))
    
    return b

"""y nuestra incognita $x = \begin{pmatrix} c_0 \\ c_1 \\ ... \\ c_n \end{pmatrix}$ que simplemente la obtenemos haciendo `lista_ci = np.linalg.solve(A, b)`

Ademas, teniendo en cuenta que $S''(x_n) = 2c_n$  y la condicion 5. se puede llegar a expresar los coeficientes cúbicos de la forma:

$d_i = \frac{1}{3h_i} (c_{i+1} - c_i)$
"""

def crear_lista_de_coeficientes_cubicos_di(h_lista: list, c_lista: list):
    nro_sub_splines = len(h_lista)
    lista_di =[]

    for i in range(nro_sub_splines):
        d_i = (1/(3*h_lista[i]))*(c_lista[i+1]-c_lista[i])
        lista_di.append(d_i)
    return lista_di

"""### **2.1.2 Definiciones de funciones auxiliares para ploteo:**

Funcion auxiliar para ploteo de subsplines:
"""

def generar_muestras_de_subspline_i(x,lista_ai: list, lista_bi: list, lista_ci: list, lista_di: list, i): 
    return (lista_ai[i] + lista_bi[i]*(x-x[0]) + lista_ci[i]*((x-x[0])**2)+lista_di[i]*((x-x[0])**3))

"""Función auxiliar para armado de las tabla con coeficientes resultantes de la interpolación:"""

def curva_dataframe(nombre,coeficientes_spline):
    lista_nombres = [[nombre,nombre,nombre,nombre],["a","b","c","d"]]
    tuplas = list(zip(*lista_nombres))
    indice = pd.MultiIndex.from_tuples(tuplas, names=["Spline", "Coeficiente"])
    tabla_de_datos = pd.DataFrame(coeficientes_spline, index=indice)
    return tabla_de_datos

"""### **2.1.3 Metodo de interpolación de Spline cúbica**

Esta función recibe la información acerca de los nodos a interpolar, y como se va a calcular interpolación por spline ligada se necesita el valor de la derivada en los nodos más extremos de la misma.
Devuelve una lista cuyos elementos son listas que contienen los coeficientes respectivos a cada subspline $i$
"""

def interpolacion_spline(nodos: list, derivada_f_x0, derivada_f_xn):
  lista_hi = crear_lista_de_incrementos_hi(nodos)
  lista_ai = crear_lista_de_coeficientes_independientes_ai(nodos)
  A = matriz_A(lista_hi)
  b = matriz_b(lista_hi, lista_ai, derivada_f_x0, derivada_f_xn)
  lista_ci = np.linalg.solve(A, b)
  lista_bi = crear_lista_de_coeficientes_lineales_bi(lista_hi, lista_ai, lista_ci)
  lista_di = crear_lista_de_coeficientes_cubicos_di(lista_hi, lista_ci)
  return [lista_ai, lista_bi, lista_ci, lista_di]

"""##### **Ejemplo** del Burden"""

#datos
tabla_de_datos_sobre_nodos = [(0,1), (1,np.e), (2,np.e**2), (3,np.e**3)] # Las tuplas significan esto -> (x, f(x))
# derivadas
derivada_f_x0 = 1
derivada_f_xn = np.e**3

lista_hi = crear_lista_de_incrementos_hi(tabla_de_datos_sobre_nodos)
lista_ai = crear_lista_de_coeficientes_independientes_ai(tabla_de_datos_sobre_nodos)

lista_hi

lista_ai

A = (matriz_A(lista_hi))
for i in range(len(A)):
    print(A[i])

b = (matriz_b(lista_hi, lista_ai, derivada_f_x0, derivada_f_xn))
b

lista_ci = np.linalg.solve(A, b)
print(lista_ci)

lista_bi = crear_lista_de_coeficientes_lineales_bi(lista_hi, lista_ai, lista_ci)
lista_bi

lista_di = crear_lista_de_coeficientes_cubicos_di(lista_hi, lista_ci)
lista_di

coeficientes_ejemplo = interpolacion_spline(tabla_de_datos_sobre_nodos,derivada_f_x0,derivada_f_xn)

interval_1 = np.linspace(0,1,400) # devuelve un arreglo de 400 valores entre 0 y 1
interval_2 = np.linspace(1,2,400) # devuelve un arreglo de 400 valores entre 1 y 2
interval_3 = np.linspace(2,3,400) # devuelve un arreglo de 400 valores entre 2 y 3

subspline_1 = generar_muestras_de_subspline_i(interval_1,coeficientes_ejemplo[0],coeficientes_ejemplo[1],coeficientes_ejemplo[2],coeficientes_ejemplo[3],0)
subspline_2 = generar_muestras_de_subspline_i(interval_2,coeficientes_ejemplo[0],coeficientes_ejemplo[1],coeficientes_ejemplo[2],coeficientes_ejemplo[3],1)
subspline_3 = generar_muestras_de_subspline_i(interval_3,coeficientes_ejemplo[0],coeficientes_ejemplo[1],coeficientes_ejemplo[2],coeficientes_ejemplo[3],2)

plt.rcParams["figure.figsize"] = [6,6]
plt.rcParams["figure.autolayout"] = True
plt.plot(interval_1,subspline_1,'b')
plt.plot(interval_2,subspline_2,'b')
plt.plot(interval_3,subspline_3,'b')
plt.grid()
plt.show()

"""## ***2.2 Planteo del problema***

### **2.2.1 Uso de datos, interpolación y tabla de coeficientes resultantes**:

Cargamos en una lista la información necesaria para la interpolación de la siguiente forma:
$(x ,f(x))$
"""

nodos_spline_1 = [(1,3),(2,3.7),(5,3.9),(6,4.2),(7,5.7),(8,6.6),(10,7.1),(13,6.7),(17,4.5)]
derivadas_spline_1 = (1,-(2/3))
nodos_spline_2 = [(17,4.5),(20,7),(23,6.1),(24,5.6),(25,5.8),(27,5.2),(27.7,4.1)]
derivadas_spline_2 = (3,-4)
nodos_spline_3 = [(27.7,4.1),(28,4.3),(29,4.1),(30,3)]
derivadas_spline_3 = (1/3,-(3/2))

"""Calculamos los coeficientes de cada spline:"""

coeficientes_spline_1 = interpolacion_spline(nodos_spline_1,derivadas_spline_1[0],derivadas_spline_1[1])
coeficientes_spline_2 = interpolacion_spline(nodos_spline_2,derivadas_spline_2[0],derivadas_spline_2[1])
coeficientes_spline_3 = interpolacion_spline(nodos_spline_3,derivadas_spline_3[0],derivadas_spline_3[1])

"""Se obtiene la tabla de valores con los coeficientes:"""

df1 = curva_dataframe("Spline 1",coeficientes_spline_1)
df2 = curva_dataframe("Spline 2",coeficientes_spline_2)
df3 = curva_dataframe("Spline 3",coeficientes_spline_3)
frames = [df1,df2,df3]
df_total = pd.concat(frames).T

df_total

"""### **2.2.2 Gráfico**

A continuación se asigna cada intervalo a graficar según cada curva y luego se calculan valores (muestras) de cada subspline para poder graficarlas:

#### Curva 1
"""

intervalo_1_curva_1 = np.linspace(1,2,100)
intervalo_2_curva_1 = np.linspace(2,5,100)
intervalo_3_curva_1 = np.linspace(5,6,100)
intervalo_4_curva_1 = np.linspace(6,7,100)
intervalo_5_curva_1 = np.linspace(7,8,100)
intervalo_6_curva_1 = np.linspace(8,10,100)
intervalo_7_curva_1 = np.linspace(10,13,100)
intervalo_8_curva_1 = np.linspace(13,17,100)

muestras_subspline_1_curva_1 = generar_muestras_de_subspline_i(intervalo_1_curva_1,coeficientes_spline_1[0],coeficientes_spline_1[1],coeficientes_spline_1[2],coeficientes_spline_1[3],0)
muestras_subspline_2_curva_1 = generar_muestras_de_subspline_i(intervalo_2_curva_1,coeficientes_spline_1[0],coeficientes_spline_1[1],coeficientes_spline_1[2],coeficientes_spline_1[3],1)
muestras_subspline_3_curva_1 = generar_muestras_de_subspline_i(intervalo_3_curva_1,coeficientes_spline_1[0],coeficientes_spline_1[1],coeficientes_spline_1[2],coeficientes_spline_1[3],2)
muestras_subspline_4_curva_1 = generar_muestras_de_subspline_i(intervalo_4_curva_1,coeficientes_spline_1[0],coeficientes_spline_1[1],coeficientes_spline_1[2],coeficientes_spline_1[3],3)
muestras_subspline_5_curva_1 = generar_muestras_de_subspline_i(intervalo_5_curva_1,coeficientes_spline_1[0],coeficientes_spline_1[1],coeficientes_spline_1[2],coeficientes_spline_1[3],4)
muestras_subspline_6_curva_1 = generar_muestras_de_subspline_i(intervalo_6_curva_1,coeficientes_spline_1[0],coeficientes_spline_1[1],coeficientes_spline_1[2],coeficientes_spline_1[3],5)
muestras_subspline_7_curva_1 = generar_muestras_de_subspline_i(intervalo_7_curva_1,coeficientes_spline_1[0],coeficientes_spline_1[1],coeficientes_spline_1[2],coeficientes_spline_1[3],6)
muestras_subspline_8_curva_1 = generar_muestras_de_subspline_i(intervalo_8_curva_1,coeficientes_spline_1[0],coeficientes_spline_1[1],coeficientes_spline_1[2],coeficientes_spline_1[3],7)

"""#### Curva 2"""

intervalo_1_curva_2 = np.linspace(17,20,100)
intervalo_2_curva_2 = np.linspace(20,23,100)
intervalo_3_curva_2 = np.linspace(23,24,100)
intervalo_4_curva_2 = np.linspace(24,25,100)
intervalo_5_curva_2 = np.linspace(25,27,100)
intervalo_6_curva_2 = np.linspace(27,27.7,100)

muestras_subspline_1_curva_2 = generar_muestras_de_subspline_i(intervalo_1_curva_2,coeficientes_spline_2[0],coeficientes_spline_2[1],coeficientes_spline_2[2],coeficientes_spline_2[3],0)
muestras_subspline_2_curva_2 = generar_muestras_de_subspline_i(intervalo_2_curva_2,coeficientes_spline_2[0],coeficientes_spline_2[1],coeficientes_spline_2[2],coeficientes_spline_2[3],1)
muestras_subspline_3_curva_2 = generar_muestras_de_subspline_i(intervalo_3_curva_2,coeficientes_spline_2[0],coeficientes_spline_2[1],coeficientes_spline_2[2],coeficientes_spline_2[3],2)
muestras_subspline_4_curva_2 = generar_muestras_de_subspline_i(intervalo_4_curva_2,coeficientes_spline_2[0],coeficientes_spline_2[1],coeficientes_spline_2[2],coeficientes_spline_2[3],3)
muestras_subspline_5_curva_2 = generar_muestras_de_subspline_i(intervalo_5_curva_2,coeficientes_spline_2[0],coeficientes_spline_2[1],coeficientes_spline_2[2],coeficientes_spline_2[3],4)
muestras_subspline_6_curva_2 = generar_muestras_de_subspline_i(intervalo_6_curva_2,coeficientes_spline_2[0],coeficientes_spline_2[1],coeficientes_spline_2[2],coeficientes_spline_2[3],5)

"""#### Curva 3"""

intervalo_1_curva_3 = np.linspace(27.7,28,100)
intervalo_2_curva_3 = np.linspace(28,29,100)
intervalo_3_curva_3 = np.linspace(29,30,100)

muestras_subspline_1_curva_3 = generar_muestras_de_subspline_i(intervalo_1_curva_3,coeficientes_spline_3[0],coeficientes_spline_3[1],coeficientes_spline_3[2],coeficientes_spline_3[3],0)
muestras_subspline_2_curva_3 = generar_muestras_de_subspline_i(intervalo_2_curva_3,coeficientes_spline_3[0],coeficientes_spline_3[1],coeficientes_spline_3[2],coeficientes_spline_3[3],1)
muestras_subspline_3_curva_3 = generar_muestras_de_subspline_i(intervalo_3_curva_3,coeficientes_spline_3[0],coeficientes_spline_3[1],coeficientes_spline_3[2],coeficientes_spline_3[3],2)

"""#### Ploteo"""

fig, ax = plt.subplots(figsize=(20, 6))


plt.rcParams["figure.autolayout"] = True

plt.title("Splines halladas con los datos de la tabla",fontsize=20)
plt.xlabel("$x$",fontsize=15)
plt.ylabel("$f(x)$", fontsize=15)


# grafico spline 1
plt.plot(intervalo_1_curva_1,muestras_subspline_1_curva_1,'r', label="Spline 1")
plt.plot(intervalo_2_curva_1,muestras_subspline_2_curva_1,'r')
plt.plot(intervalo_3_curva_1,muestras_subspline_3_curva_1,'r')
plt.plot(intervalo_4_curva_1,muestras_subspline_4_curva_1,'r')
plt.plot(intervalo_5_curva_1,muestras_subspline_5_curva_1,'r')
plt.plot(intervalo_6_curva_1,muestras_subspline_6_curva_1,'r')
plt.plot(intervalo_7_curva_1,muestras_subspline_7_curva_1,'r')
plt.plot(intervalo_8_curva_1,muestras_subspline_8_curva_1,'r')
# grafico spline 2
plt.plot(intervalo_1_curva_2,muestras_subspline_1_curva_2,'g', label="Spline 2")
plt.plot(intervalo_2_curva_2,muestras_subspline_2_curva_2,'g')
plt.plot(intervalo_3_curva_2,muestras_subspline_3_curva_2,'g')
plt.plot(intervalo_4_curva_2,muestras_subspline_4_curva_2,'g')
plt.plot(intervalo_5_curva_2,muestras_subspline_5_curva_2,'g')
plt.plot(intervalo_6_curva_2,muestras_subspline_6_curva_2,'g')
# grafico spline 3
plt.plot(intervalo_1_curva_3,muestras_subspline_1_curva_3,'b', label="Spline 3")
plt.plot(intervalo_2_curva_3,muestras_subspline_2_curva_3,'b')
plt.plot(intervalo_3_curva_3,muestras_subspline_3_curva_3,'b')

# graficamos los puntos dados por el ejercicio

label_ploteado = False
for x,y in nodos_spline_1:
    if label_ploteado:
        plt.plot(x,y,'ro')
    else:
        plt.plot(x,y,'ro', label = "Nodos spline 1")
        label_ploteado = True

label_ploteado = False
for x,y in nodos_spline_2:
    if label_ploteado:
        plt.plot(x,y,'go')
    else:
        plt.plot(x,y,'go', label = "Nodos spline 2")
        label_ploteado = True

label_ploteado = False
for x,y in nodos_spline_3:
    if label_ploteado:
        plt.plot(x,y,'bo')
    else:
        plt.plot(x,y,'bo', label = "Nodos spline 1")
        label_ploteado = True



ax.xaxis.set_major_locator(MultipleLocator(1))
ax.yaxis.set_major_locator(MultipleLocator(1))
ax.set_xlim(0, 32)
ax.set_ylim(0, 8)

plt.legend()
ax.grid()
plt.show()

"""### **2.2.3 Conclusiones**:

Se puede observar que la aproximación realizada en general resulta en una buena aproximación de las funciones interpoladas, aunque hay que destacar que se nota una levemente menor precisión de dicha aproximación para la curva 2 a comparación de las demás.

# ***3. Conclusiones generales***

A lo largo del trabajo se desarrollan y se alcanzan los objetivos propuestos al inicio del mismo, corroborando que los resultados teóricos se ven reflejados en planteo realizado y en la aplicación de métodos para estos problemas en particular.

# ***4. Referencias***

- Burden, R.L., Faires, J.D., Análisis Numérico. Grupo Editorial Iberoamericano, 1985.

- Apuntes del curso Análisis numérico 1 - curso Sassano - Facultad de Ingeniería - Universidad de Buenos Aires - 2022. 

- Material audiovisual del curso Análisis numérico 1 - curso Sassano - Facultad de Ingeniería - Universidad de Buenos Aires - 2022. 

- Manual de Latex,  https://manualdelatex.com/ .
"""